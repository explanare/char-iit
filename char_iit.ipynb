{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "eEzPQmcEVktn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install datasets\n",
        "pip install transformers\n",
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "9dcB1BONbDLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RygV6WwtZ-v"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "tar -xzf data.tgz\n",
        "tar -xzf tokenizers.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "DATA_DIR = 'data'\n",
        "MODEL_DIR = 'models'\n",
        "\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "  pathlib.Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "if not os.path.isdir(MODEL_DIR):\n",
        "  pathlib.Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print(os.listdir(DATA_DIR))\n",
        "print(os.listdir(MODEL_DIR))\n",
        "\n",
        "# Change huggingface cache directories.\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.path.join(DATA_DIR, 'hf_cache')\n",
        "os.environ['HF_DATASETS_CACHE'] = os.path.join(DATA_DIR, 'hf_cache')"
      ],
      "metadata": {
        "id": "-cTJto9HTVT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "al5EXbhaVi4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "NMBK0fPHZV1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown T5 tokenizer\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "t5_default_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", cache_dir=MODEL_DIR)\n",
        "tokenizer = copy.deepcopy(t5_default_tokenizer)\n",
        "\n",
        "VOCAB = list(sorted(tokenizer.get_vocab(), key=tokenizer.get_vocab().get))\n",
        "SPM_SPACE = 'â–'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-6wNmDgLMNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IIT Data Generation"
      ],
      "metadata": {
        "id": "ex-BkjKONm3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character annotations"
      ],
      "metadata": {
        "id": "w_23rSWgSCs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Pre-compute character annotations for interventions in TSV format.\n",
        "\n",
        "# Each line has the format\n",
        "#     \"{input}\\t{label}\\t{feature}\\t{anno_0}\\t{anno_1}...\\t{anno_n}\",\n",
        "# where {feature} is a substring of input to apply character interventions.\n",
        "# For most tasks, feature equals to input.\n",
        "# {anno_i} is the character annotation in the format\n",
        "#     \"{token_index}\\t{char_pos}\\t{char_val}\"\n",
        "\n",
        "\n",
        "import collections\n",
        "\n",
        "def get_pos_to_token_index(text):\n",
        "  \"\"\"Map char index to SPM token index.\"\"\"\n",
        "  token_ids = tokenizer(text).input_ids\n",
        "  tokens = [VOCAB[i] for i in token_ids if i != 1]\n",
        "  pos_to_tokens = {}\n",
        "  curr_pos = 0\n",
        "  for i, tok in enumerate(tokens):\n",
        "    if i == 0:\n",
        "      # remove the leading SOS space.\n",
        "      tok = tok.lstrip(SPM_SPACE)\n",
        "    pos_to_tokens.update({pos: i for pos in range(curr_pos, curr_pos + len(tok))})\n",
        "    curr_pos += len(tok)\n",
        "  return pos_to_tokens, tokens\n",
        "\n",
        "\n",
        "# Get character locations, specified by the token index and position of the\n",
        "# char in the token.\n",
        "# Each char is a candidate for intervention, which will be mapped to a fix\n",
        "# sized dimensions in the transformer representations.\n",
        "def get_char_level_inv_locs(input_tokens, span, include_space=False):\n",
        "  \"\"\"Returns a set of possible intervention locations.\"\"\"\n",
        "  inv_loc_candidates = [\n",
        "        (i, c_i) for i in range(*span) for c_i in range(len(input_tokens[i]))\n",
        "        # If include space, exclude the extra leading space added by the\n",
        "        # tokenizer only, otherwise exclude all spaces.\n",
        "        if (include_space and not (\n",
        "            i == span[0] and c_i == 0 and input_tokens[i][c_i] == SPM_SPACE)) or\n",
        "           (not include_space and input_tokens[i][c_i] != SPM_SPACE)]\n",
        "  return inv_loc_candidates\n",
        "\n",
        "\n",
        "def match_feature_span(input_text, feature_text):\n",
        "  # Return the first occurrence.\n",
        "  span_begin = input_text.find(feature_text)\n",
        "  if span_begin < 0:\n",
        "    return None\n",
        "  return span_begin\n",
        "\n",
        "\n",
        "def gen_char_level_feature_annotation_tsv(\n",
        "    input_tsv_path, output_tsv_path, task_name, feature_to_column, inv_config):\n",
        "  assert input_tsv_path != output_tsv_path\n",
        "  assert ('input' in feature_to_column and 'label' in feature_to_column and\n",
        "          'feature' in feature_to_column)\n",
        "  stats = collections.defaultdict(int)\n",
        "  with open(output_tsv_path, 'w') as f_out:\n",
        "    with open(input_tsv_path, 'r') as f:\n",
        "      for line in f:\n",
        "        stats['total'] += 1\n",
        "        parsed = line.strip().split('\\t')\n",
        "        input_text = parsed[feature_to_column['input']]\n",
        "        label = parsed[feature_to_column['label']]\n",
        "        feature_text = parsed[\n",
        "            feature_to_column['feature'] if feature_to_column['feature'] < len(parsed) else 0]\n",
        "        # Find the substring to apply character intervention.\n",
        "        span_begin = match_feature_span(input_text, feature_text)\n",
        "        if span_begin is None:\n",
        "          stats['span_not_found'] += 1\n",
        "          print('SPAN_NOT_FOUND:', input_text, feature_text)\n",
        "          continue\n",
        "        pos_to_token_index, input_tokens = get_pos_to_token_index(input_text)\n",
        "        # Append an extra position to handle the end of span\n",
        "        pos_to_token_index[len(input_text)] = len(input_tokens)\n",
        "        # Check if the span is matched corretly, if not drop it.\n",
        "        if (span_begin not in pos_to_token_index or\n",
        "            span_begin + len(feature_text) not in pos_to_token_index):\n",
        "          stats['span_not_in_pos_index'] += 1\n",
        "          print('SPAN_NOT_IN_INDEX:', span_begin, span_begin + len(feature_text), input_text, feature_text)\n",
        "          continue\n",
        "        span_index = (pos_to_token_index[span_begin],\n",
        "                      pos_to_token_index[span_begin + len(feature_text)])\n",
        "        span_reconstruct = ''.join(\n",
        "            input_tokens[span_index[0]:span_index[1]]).replace(SPM_SPACE, ' ').strip()\n",
        "        if span_reconstruct != feature_text:\n",
        "          stats['span_mismatch'] += 1\n",
        "          print('MISMATCH:', feature_text, '|', span_reconstruct)\n",
        "          continue\n",
        "        # Annotated each char with its location and value.\n",
        "        inv_loc_candidates = get_char_level_inv_locs(\n",
        "            input_tokens, span_index, include_space=inv_config['include_space'])\n",
        "        # Remove any char pos that is greater than 16 (i.e. subword pieces with\n",
        "        # more than 16 characters, which should not happen for T5 vocab).\n",
        "        if max([loc[1] for loc in inv_loc_candidates]) >= 16:\n",
        "          stats['char_pos_exceed_16'] += 1\n",
        "          print('CHAR_POS_EXCEED_16:', feature_text)\n",
        "          continue\n",
        "        # Sanity check that the number of chars annotated matches the\n",
        "        if (inv_config['include_space'] and len(inv_loc_candidates) != len(feature_text)) or (\n",
        "            not inv_config['include_space'] and len(inv_loc_candidates) != len(feature_text.replace(' ', ''))):\n",
        "          print(feature_text)\n",
        "          print(input_tokens[span_index[0]:span_index[1]])\n",
        "          print(inv_loc_candidates)\n",
        "          continue\n",
        "        char_annotations = [loc + (input_tokens[loc[0]][loc[1]],) for loc in inv_loc_candidates]\n",
        "        # Truncate or pad to max number of annotations.\n",
        "        char_annotations = char_annotations[:inv_config['max_num_anno']]\n",
        "        char_annotations.extend(\n",
        "            [(-1, -1, '')] * (inv_config['max_num_anno'] - len(char_annotations)))\n",
        "        data = [f'{input_text}\\t{label}\\t{task_name}\\t{feature_text}'] + [\n",
        "            '\\t'.join(map(str, inv)) for inv in char_annotations]\n",
        "        f_out.write('\\t'.join(data) + '\\n')\n",
        "        stats['annotated'] += 1\n",
        "  return dict(stats)\n",
        "\n",
        "\n",
        "def gen_character_annotation_data(input_tsv_path_base, task_name,\n",
        "                                  inv_config, splits=None):\n",
        "  splits = splits or ['train', 'val']\n",
        "  # The input format is either \"{input}\\t{output}\" or\n",
        "  # \"{input}\\t{output}\\t{task}\\t{template}\\t{feature}\".\n",
        "  feature_to_column = {'input': 0, 'label': 1, 'feature': 4}\n",
        "  dataset_split_to_path = {\n",
        "      split: input_tsv_path_base % (task_name + (f'_{split}' if split else ''))\n",
        "      for split in splits}\n",
        "  anno_dataset_split_to_path = {}\n",
        "  for split in dataset_split_to_path:\n",
        "    span_annotated_tsv_path = dataset_split_to_path[split].replace(\n",
        "        '.tsv', '_char_anno.tsv')\n",
        "    anno_dataset_split_to_path[split] = span_annotated_tsv_path\n",
        "    stats = gen_char_level_feature_annotation_tsv(\n",
        "        dataset_split_to_path[split],\n",
        "        span_annotated_tsv_path,\n",
        "        task_name,\n",
        "        feature_to_column,\n",
        "        inv_config)\n",
        "    print(split, stats)\n",
        "    !wc -l $span_annotated_tsv_path\n",
        "    !head $span_annotated_tsv_path\n",
        "  return anno_dataset_split_to_path"
      ],
      "metadata": {
        "id": "9GDjSdaXP17v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gnerate intervention examples"
      ],
      "metadata": {
        "id": "IQ5vDyb9k9Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Simulate effects of character interventions for pure form-based tasks.\n",
        "\n",
        "\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "def intervene_reversal_base_example(base_example, max_num_inv):\n",
        "  # Select a set of chars to keep and randomly assign values to others\n",
        "  base_val = base_example['feature']\n",
        "  num_char_inv = random.randint(1, min(max_num_inv, len(base_val)))\n",
        "  num_char_keep = len(base_val) - num_char_inv\n",
        "  kept_pos = random.sample(range(len(base_val)), k=num_char_keep)\n",
        "  inv_feature = ''.join([base_val[i] if i in kept_pos else random.choice(string.ascii_lowercase)\n",
        "                  for i in range(len(base_val))])\n",
        "  return {\n",
        "      'input': inv_feature,\n",
        "      'feature': inv_feature,\n",
        "      'label': inv_feature[::-1],\n",
        "      'inv_locs': [(None, None, c) for c in inv_feature]}\n",
        "\n",
        "\n",
        "def shift_digit(num_str, shift):\n",
        "  int_str, frac_str = num_str, ''\n",
        "  if '.' in num_str:\n",
        "    int_str, frac_str = num_str.split('.')\n",
        "  if shift >= 0:\n",
        "    if shift < len(frac_str):\n",
        "      num_str = ((int_str + frac_str[:shift]).lstrip('0') or '0') + '.' + frac_str[shift:]\n",
        "    else:\n",
        "      num_str = (int_str + frac_str + '0' * (shift - len(frac_str))).lstrip('0') or '0'\n",
        "  else:\n",
        "    shift = abs(shift)\n",
        "    if shift < len(int_str):\n",
        "      num_str = int_str[:len(int_str)-shift] + '.' + int_str[-shift:] + frac_str\n",
        "    else:\n",
        "      num_str = '0.' + '0' * (shift - len(int_str)) + int_str + frac_str\n",
        "    num_str = num_str.rstrip('0').rstrip('.')\n",
        "  return num_str\n",
        "\n",
        "\n",
        "def intervene_unit_conversion_base_example(base_example):\n",
        "  # Get source and target numbers to determine the unit conversion in the base example.\n",
        "  base_src_number, base_trg_number = base_example['feature'], base_example['label']\n",
        "  inv_int_digits = ''.join(\n",
        "      random.choices(string.digits, k=random.randint(1, 4))).lstrip('0') or '0'\n",
        "  inv_frac_digits = ''.join(\n",
        "      random.choices(string.digits, k=random.randint(1, 3))).rstrip('0') or ''\n",
        "  inv_src_number = inv_int_digits\n",
        "  if inv_frac_digits:\n",
        "    inv_src_number += '.' + inv_frac_digits\n",
        "  shift = (round(math.log10(float(base_trg_number) / float(base_src_number)))\n",
        "          if base_trg_number >= base_src_number else\n",
        "           -round(math.log10(float(base_src_number) / float(base_trg_number))))\n",
        "  inv_trg_number = shift_digit(inv_src_number, shift)\n",
        "  return {\n",
        "      'input': base_example['input'].replace(base_src_number, inv_src_number),\n",
        "      'feature': inv_src_number,\n",
        "      'label': inv_trg_number,\n",
        "      'inv_locs': [(None, None, c) for c in inv_src_number]}\n",
        "\n",
        "\n",
        "# For pure form-based tasks where no lexicon is involved.\n",
        "def gen_form_only_inv_example(base_example, max_num_inv, task):\n",
        "  if task == 'reversal':\n",
        "    inv_example = intervene_reversal_base_example(base_example, max_num_inv)\n",
        "  elif task == 'unit_conversion':\n",
        "    inv_example = intervene_unit_conversion_base_example(base_example)\n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "  return inv_example\n",
        "\n",
        "\n",
        "# Test\n",
        "def run_test():\n",
        "  print(gen_form_only_inv_example({'input': 'abcdef', 'label': 'fedcba', 'feature': 'abcdef'},\n",
        "                                   max_num_inv=8, task='reversal'))\n",
        "  # Possible outputs:\n",
        "  # {'inv_input': 'avcdgf',\n",
        "  #  'inv_feature': 'avcdgf',\n",
        "  #  'inv_label': 'fgdcva',\n",
        "  #  'inv_locs': [(None, None, 'a'),\n",
        "  #   (None, None, 'v'),\n",
        "  #   (None, None, 'c'),\n",
        "  #   (None, None, 'd'),\n",
        "  #   (None, None, 'g'),\n",
        "  #   (None, None, 'f')]}\n",
        "  print(gen_form_only_inv_example({'input': 'convert 123 million to billion', 'label': '0.123', 'feature': '123'},\n",
        "                                    max_num_inv=8, task='unit_conversion'))\n",
        "  # Possible outputs:\n",
        "  # {'inv_input': 'convert 6.9 million to billion',\n",
        "  #  'inv_feature': '6.9',\n",
        "  #  'inv_label': '0.0069',\n",
        "  #  'inv_locs': [(None, None, '6'), (None, None, '.'), (None, None, '9')]}"
      ],
      "metadata": {
        "id": "5V2HNEZgvKKx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown IIT Data Generation (The algorithm in Appendix A.1)\n",
        "\n",
        "import itertools\n",
        "import math\n",
        "import string\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "ANNO_LEN = 3\n",
        "\n",
        "def parse_anno_line(line):\n",
        "  \"\"\"Parse character annotations.\n",
        "    Each line has the format\n",
        "        \"{input}\\t{label}\\t{task}\\t{feature}\\t{anno_0}\\t{anno_1}...\\t{anno_n}\",\n",
        "    where {feature} is a substring of input to apply character interventions.\n",
        "    For most tasks, feature equals to input.\n",
        "    {anno_i} is the character annotation in the format\n",
        "        \"{token_index}\\t{char_pos}\\t{char_val}\"\n",
        "  \"\"\"\n",
        "  parsed = line.strip('\\n').split('\\t')\n",
        "  loc_offset = 4\n",
        "  num_anno = (len(parsed) - loc_offset) // ANNO_LEN\n",
        "  return {'input': parsed[0], 'label': parsed[1], 'feature': parsed[3],\n",
        "          'inv_locs': [[\n",
        "              # (token_index, char_pos)\n",
        "              int(parsed[loc_offset + offset + i * ANNO_LEN]) for offset in range(ANNO_LEN - 1)] + [\n",
        "              # char_val\n",
        "              parsed[loc_offset + ANNO_LEN - 1 + i * ANNO_LEN]]\n",
        "              for i in range(num_anno)] + [[-1, -1, '']]}\n",
        "\n",
        "\n",
        "def get_all_inv_val(example):\n",
        "  \"\"\"Return the sequence of char values for interventions.\"\"\"\n",
        "  # The inv value is different from feature if SPM_SPACE in feature.\n",
        "  return ''.join([loc[-1] for loc in example['inv_locs'] if loc[0] != -1]).lower()\n",
        "\n",
        "\n",
        "def get_template_val(example, task):\n",
        "  # For most tasks, the template is an empty string.\n",
        "  # For unit_conversion, the template is the context that specifies which\n",
        "  # operation to perform.\n",
        "  template = example['input'].replace(example['feature'], '{feature}')\n",
        "  if task == 'spelling_correction_contextual':\n",
        "    template = template.lower()\n",
        "  return template\n",
        "\n",
        "\n",
        "def build_char_to_feature_index(feature_to_example_index, key_type):\n",
        "  if key_type == 'char':\n",
        "    char_to_feature_index = collections.defaultdict(set)\n",
        "    for feat_val in feature_to_example_index:\n",
        "      for c in feat_val:\n",
        "        char_to_feature_index[c].add(feat_val)\n",
        "  elif key_type.startswith('char'):\n",
        "    # Cache more all keys of length n for faster indexing.\n",
        "    n = int(key_type.replace('char', ''))\n",
        "    ordered_feature_index = list(feature_to_example_index.keys())\n",
        "    char_to_feature_index = collections.defaultdict(set)\n",
        "    for feat_i, feat_val in enumerate(ordered_feature_index):\n",
        "      key = sorted(set(feat_val))\n",
        "      for subset in itertools.combinations(key, n):\n",
        "        char_to_feature_index[''.join(subset)].add(feat_i)\n",
        "        # Avoid OOM.\n",
        "        if len(char_to_feature_index) > 10_000:\n",
        "          print(list(char_to_feature_index.keys())[:20])\n",
        "          raise ValueError\n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "  print('char_to_feature_index:', 'len=%d' % len(char_to_feature_index),\n",
        "        {c: len(char_to_feature_index[c]) for c in sorted(char_to_feature_index)})\n",
        "  char_to_feature_index = dict(char_to_feature_index)\n",
        "  return char_to_feature_index if key_type == 'char' else (char_to_feature_index, ordered_feature_index)\n",
        "\n",
        "\n",
        "# Optimize the char lookup order from least frequent to most frequent\n",
        "# for faster indexing.\n",
        "letter_sorted_by_freq = dict(zip(\n",
        "    [' ', 'e', 't', 'a', 'i', 'n', 'o', 's', 'h', 'r', 'd', 'l', 'u', 'c', 'm',\n",
        "         'f', 'w', 'y', 'g', 'p', 'b', 'v', 'k', 'q', 'j', 'x', 'z',\n",
        "     '9', '8', '7', '6', '5', '4', '3', '2', '1', '0', ',', '.', '-', \"'\"],\n",
        "    range(41)))\n",
        "\n",
        "\n",
        "def get_features_containing_chars(required_chars, char_to_feature_index, key_type):\n",
        "  if key_type == 'char':\n",
        "    required_chars = sorted(set(required_chars), key=lambda k: len(char_to_feature_index[k]))\n",
        "    # Run time optimization by computing set intersection over smallest sets first.\n",
        "    source_val_candids = list(\n",
        "        set.intersection(*[char_to_feature_index[c] for c in required_chars]))\n",
        "  elif key_type.startswith('char'):\n",
        "    n = int(key_type.replace('char', ''))\n",
        "    char_to_feature_index, ordered_feature_index = char_to_feature_index\n",
        "    required_chars = sorted(set(required_chars), key=letter_sorted_by_freq.get, reverse=True)\n",
        "    required_char_paris = [''.join(sorted(required_chars[i:i+n]))\n",
        "                           for i in range(0, len(required_chars) - (len(required_chars) % n), n)]\n",
        "    if len(required_chars) % n:\n",
        "      padded_chars = (required_chars[-n:] if len(required_chars[-n:]) == n else\n",
        "                      required_chars + [' '])\n",
        "      required_char_paris.append(''.join(sorted(padded_chars)))\n",
        "    required_char_paris = sorted(required_char_paris, key=lambda k: len(char_to_feature_index[k]))\n",
        "    source_feat_index_candids = list(\n",
        "        set.intersection(*[char_to_feature_index[p] for p in required_char_paris]))\n",
        "    source_val_candids = [ordered_feature_index[i] for i in source_feat_index_candids]\n",
        "  return source_val_candids\n",
        "\n",
        "\n",
        "def generate_triplet_index(examples, max_num_triplet, max_num_inv, task_name):\n",
        "  feature_to_example_index = collections.defaultdict(list)\n",
        "  for i in range(len(examples)):\n",
        "    feature_to_example_index[get_all_inv_val(examples[i])].append(i)\n",
        "  print('#unique_index_feature=%d' % len(feature_to_example_index))\n",
        "  # For most tasks without a template, the index has a single key which is the\n",
        "  # empty string.\n",
        "  template_to_example_index = collections.defaultdict(list)\n",
        "  for i in range(len(examples)):\n",
        "    template_to_example_index[get_template_val(examples[i], task_name)].append(i)\n",
        "  print('#unique_template=%d' % len(template_to_example_index))\n",
        "  key_type = 'char2' if task_name == 'spelling_correction_contextual' else 'char'\n",
        "  char_to_feature_index = build_char_to_feature_index(\n",
        "      feature_to_example_index, key_type)\n",
        "  stats = collections.defaultdict(int)\n",
        "  max_num_triplet_per_base = int(max_num_triplet / len(examples)) + 1\n",
        "  print(f'max_num_triplet_per_base={max_num_triplet_per_base}')\n",
        "  for base_i in range(len(examples)):\n",
        "    base_val = get_all_inv_val(examples[base_i])\n",
        "    num_triplet_per_base = 0\n",
        "    template = get_template_val(examples[base_i], task_name)\n",
        "    inv_index_candids = template_to_example_index[template]\n",
        "    # Skip base with only one possible intervention value.\n",
        "    if len(inv_index_candids) == 1:\n",
        "      stats['single_inv_candid_for_base'] += 1\n",
        "      continue\n",
        "    inv_index_samples = random.sample(\n",
        "        inv_index_candids, min(len(inv_index_candids), max_num_triplet_per_base * 2))\n",
        "    num_unique_inv_label = len(set([examples[i][\"label\"] for i in inv_index_samples]))\n",
        "    max_num_triplet_per_inv = math.ceil(max_num_triplet_per_base / num_unique_inv_label)\n",
        "    for inv_i in inv_index_samples:\n",
        "      num_triplet_per_inv = 0\n",
        "      inv_val = get_all_inv_val(examples[inv_i])\n",
        "      required_chars = [inv_val[i] for i in range(len(inv_val))\n",
        "                        if i >= len(base_val) or base_val[i] != inv_val[i]]\n",
        "      if len(required_chars) > max_num_inv:\n",
        "        stats['exceed_max_require_chars'] += 1\n",
        "        continue\n",
        "      if len(required_chars) == 0 or len(required_chars) == 1:\n",
        "        stats['base_inv_too_similar'] += 1\n",
        "        continue\n",
        "      source_val_candids = get_features_containing_chars(\n",
        "          required_chars, char_to_feature_index, key_type)\n",
        "      if not source_val_candids:\n",
        "        stats['no_source_candidates'] += 1\n",
        "      for _ in range(len(source_val_candids)):\n",
        "        source_val = random.choice(source_val_candids)\n",
        "        for source_i in random.sample(feature_to_example_index[source_val], 1):\n",
        "          if source_i == inv_i:\n",
        "            continue\n",
        "          yield (base_i, source_i, inv_i)\n",
        "          stats['valid'] += 1\n",
        "          num_triplet_per_base += 1\n",
        "          num_triplet_per_inv += 1\n",
        "          if stats['valid'] >= max_num_triplet:\n",
        "            print(dict(stats))\n",
        "            return\n",
        "        if num_triplet_per_base >= max_num_triplet_per_base or num_triplet_per_inv >= max_num_triplet_per_inv:\n",
        "          break\n",
        "      if num_triplet_per_base >= max_num_triplet_per_base:\n",
        "        break\n",
        "  print(dict(stats))\n",
        "  return\n",
        "\n",
        "\n",
        "def generate_form_only_triplet_index(examples, max_num_triplet, max_num_inv, task_name):\n",
        "  feature_to_example_index = collections.defaultdict(list)\n",
        "  for i in range(len(examples)):\n",
        "    feature_to_example_index[get_all_inv_val(examples[i])].append(i)\n",
        "  print('#unique_index_feature=%d' % len(feature_to_example_index))\n",
        "  key_type = 'char'\n",
        "  char_to_feature_index = build_char_to_feature_index(\n",
        "      feature_to_example_index, key_type)\n",
        "\n",
        "  stats = collections.defaultdict(int)\n",
        "  max_num_triplet_per_base = int(max_num_triplet / len(examples)) + 1\n",
        "  print(f'max_num_triplet_per_base={max_num_triplet_per_base}')\n",
        "  for base_i in range(len(examples)):\n",
        "    base_example = examples[base_i]\n",
        "    base_val = get_all_inv_val(base_example)\n",
        "    num_triplet_per_base = 0\n",
        "    for _ in range(max_num_triplet_per_base * 2):\n",
        "      inv_example = gen_form_only_inv_example(base_example, 8, task_name)\n",
        "      inv_val = get_all_inv_val(inv_example)\n",
        "      required_chars = [inv_val[i] for i in range(len(inv_val))\n",
        "                        if i >= len(base_val) or base_val[i] != inv_val[i]]\n",
        "      if len(required_chars) > max_num_inv or len(required_chars) == 0:\n",
        "        stats['invalid_require_chars'] += 1\n",
        "        continue\n",
        "      source_val_candids = get_features_containing_chars(\n",
        "          required_chars, char_to_feature_index, key_type)\n",
        "      if not source_val_candids:\n",
        "        stats['no_source_candidates'] += 1\n",
        "      num_triplet_per_inv = 0\n",
        "      for _ in range(len(source_val_candids)):\n",
        "        source_val = random.choice(source_val_candids)\n",
        "        for source_i in random.sample(feature_to_example_index[source_val], 1):\n",
        "          num_triplet_per_inv += 1\n",
        "          yield (base_example, source_i, inv_example)\n",
        "          stats['valid'] += 1\n",
        "          num_triplet_per_base += 1\n",
        "          if stats['valid'] >= max_num_triplet:\n",
        "            print(dict(stats))\n",
        "            return\n",
        "          if num_triplet_per_base >= max_num_triplet_per_base or num_triplet_per_inv >= 1:\n",
        "            break\n",
        "        if num_triplet_per_base >= max_num_triplet_per_base or num_triplet_per_inv >= 1:\n",
        "          break\n",
        "      if num_triplet_per_base >= max_num_triplet_per_base:\n",
        "        break\n",
        "  print(dict(stats))\n",
        "  return\n",
        "\n",
        "\n",
        "def test_generate_triplet_index(anno_input_tsv_path, iit_config, task_name):\n",
        "  with open(anno_input_tsv_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    examples = list(map(lambda x: parse_anno_line(x), lines))\n",
        "  print(f'Parsed {len(examples)} examples.')\n",
        "  #stats = collections.defaultdict(int)\n",
        "  for base_index, source_index, inv_index in generate_triplet_index(\n",
        "      examples, max_num_triplet=20, max_num_inv=iit_config['max_num_inv'], task_name=task_name):\n",
        "    base_val = examples[base_index]['feature']\n",
        "    source_val = examples[source_index]['feature']\n",
        "    inv_val = examples[inv_index]['feature']\n",
        "    print(\"BASE  :\", base_val, '|', examples[base_index]['label'])\n",
        "    print(\"INV   :\", inv_val, '|', examples[inv_index]['label'])\n",
        "    print(\"SOURCE:\", source_val, '|', examples[source_index]['label'])\n",
        "    #stats[(len(examples[base_index]['label']) < 48, len(examples[source_index]['label']) < 48)] += 1\n",
        "    for i, c in enumerate(inv_val):\n",
        "      assert c.lower() in source_val.lower() or (\n",
        "          i < len(base_val) and base_val[i].lower() == inv_val[i].lower()), (i, c)\n",
        "  #print(dict(stats))\n",
        "\n",
        "\n",
        "def test_generate_form_only_triplet_index(anno_input_tsv_path, iit_config, task_name):\n",
        "  with open(anno_input_tsv_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    examples = list(map(lambda x: parse_anno_line(x), lines))\n",
        "  print(f'Parsed {len(examples)} examples.')\n",
        "  for base_example, source_index, inv_example in generate_form_only_triplet_index(\n",
        "      examples, max_num_triplet=20, max_num_inv=iit_config['max_num_inv'], task_name=task_name):\n",
        "    base_val = base_example['feature']\n",
        "    source_val = examples[source_index]['feature']\n",
        "    inv_val = inv_example['feature']\n",
        "    print(\"BASE  :\", base_val, '|', base_example['label'])\n",
        "    print(\"INV   :\", inv_val, '|', inv_example['label'])\n",
        "    print(\"SOURCE:\", source_val, '|', examples[source_index]['label'])\n",
        "    for i, c in enumerate(inv_val):\n",
        "      assert c in source_val or (i < len(base_val) and base_val[i] == inv_val[i]), (i, c)\n",
        "\n",
        "\n",
        "# For testing\n",
        "\n",
        "#task_name = 'reversal'\n",
        "#test_generate_triplet_index(\n",
        "#    anno_dataset_split_to_path['train'], TASK_TO_IIT_DATA[task_name]['iit_config'], task_name)\n",
        "#test_generate_form_only_triplet_index(\n",
        "#    anno_dataset_split_to_path['train'], TASK_TO_IIT_DATA[task_name]['iit_config'], task_name)"
      ],
      "metadata": {
        "id": "NWu3zcdonun1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "MAX_CHAR_PER_TOKEN = 16\n",
        "\n",
        "def get_pad_locations(source_inv_locs):\n",
        "  loc_to_pos = collections.defaultdict(list)\n",
        "  for i, loc in enumerate(source_inv_locs):\n",
        "    if loc[0] == -1:\n",
        "      break\n",
        "    loc_to_pos[loc[0]].append(loc[1])\n",
        "  return [(loc, max(loc_to_pos[loc])) for loc in loc_to_pos\n",
        "          if max(loc_to_pos[loc]) < MAX_CHAR_PER_TOKEN]\n",
        "\n",
        "\n",
        "# Interventions that preserve source distribution\n",
        "def gen_inv_base_and_source_features(\n",
        "    base_example, source_example, inv_example, max_num_inv):\n",
        "  # Find inv locations in source.\n",
        "  inv_val = get_all_inv_val(inv_example)\n",
        "  base_val = get_all_inv_val(base_example)\n",
        "  source_char_to_location = {}\n",
        "  for i, source_loc in enumerate(source_example['inv_locs']):\n",
        "    if source_loc[0] < 0:\n",
        "      break\n",
        "    source_char_to_location[source_loc[-1].lower()] = source_loc[:-1]\n",
        "  source_char_to_location['<pad>'] = get_pad_locations(source_example['inv_locs'])\n",
        "  # Find inv locations in base.\n",
        "  base_locations = []\n",
        "  source_locations = []\n",
        "  for i, base_loc in enumerate(base_example['inv_locs']):\n",
        "    if base_loc[0] < 0:\n",
        "      break\n",
        "    if (i < len(inv_val) and base_val[i] != inv_val[i]) or (\n",
        "        base_val == inv_val and inv_val[i] in source_char_to_location):\n",
        "      if inv_val[i] not in source_char_to_location:\n",
        "        print(base_val)\n",
        "        print(inv_val)\n",
        "        print(inv_val[i], i)\n",
        "        print(source_example['feature'])\n",
        "        print(source_char_to_location)\n",
        "      source_loc = source_char_to_location[inv_val[i]]\n",
        "      base_locations.append(base_loc[:-1])\n",
        "      source_locations.append(source_loc)\n",
        "    elif i >= len(inv_val):\n",
        "      # base has extra char.\n",
        "      if not source_char_to_location['<pad>']:\n",
        "        return None\n",
        "      source_loc = random.choice(source_char_to_location['<pad>'])\n",
        "      source_locations.append(source_loc)\n",
        "      base_locations.append(base_loc[:-1])\n",
        "    curr_loc = base_loc\n",
        "  # inv has extra char\n",
        "  for i in range(len(base_val), len(inv_val)):\n",
        "    source_locations.append(source_char_to_location[inv_val[i]])\n",
        "    # Append positions after the last char.\n",
        "    # Check if it goes over 16 chars.\n",
        "    if curr_loc[1] >= MAX_CHAR_PER_TOKEN - 1:\n",
        "      return None\n",
        "    base_locations.append((curr_loc[0], curr_loc[1] + 1))\n",
        "    curr_loc = base_locations[-1]\n",
        "  assert max([loc[-1] for loc in base_locations]) < MAX_CHAR_PER_TOKEN\n",
        "  assert len(base_locations) == len(source_locations)\n",
        "  base_locations.extend([[-1, -1]] * (max_num_inv - len(base_locations)))\n",
        "  source_locations.extend([[-1, -1]] * (max_num_inv - len(source_locations)))\n",
        "  return {'base_input': base_example['input'],\n",
        "          'source_input': source_example['input'],\n",
        "          'base_label': base_example['label'], # for debugging\n",
        "          'source_label': source_example['label'], # for debugging\n",
        "          'inv_label': inv_example['label'], # for debugging\n",
        "          'inv_input': inv_example['input'], # for debugging\n",
        "          'base_locations': base_locations,\n",
        "          'source_locations': source_locations}"
      ],
      "metadata": {
        "id": "pH0F70w0kVdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def gen_inv_examples(anno_input_tsv_path, output_tsv_path, num_examples, task_name):\n",
        "  iit_config = TASK_TO_IIT_DATA[task_name]['iit_config']\n",
        "  max_input_seq_len = TASK_TO_DATASETS[task_name]['seq_length']['max_src_token']\n",
        "  label_dist = collections.defaultdict(int)\n",
        "  with open(anno_input_tsv_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    examples = list(map(lambda x: parse_anno_line(x), lines))\n",
        "  print(f'Parsed {len(examples)} examples.')\n",
        "  is_form_only = task_name in ('reversal', 'unit_conversion')\n",
        "  triplet_gen_fn = generate_form_only_triplet_index if is_form_only else generate_triplet_index\n",
        "  triplet_gen = triplet_gen_fn(\n",
        "        examples, max_num_triplet=num_examples,\n",
        "        max_num_inv=iit_config['max_num_inv'], task_name=task_name)\n",
        "  inv_examples = []\n",
        "  unique_inv_label_per_base = collections.defaultdict(set)\n",
        "  unique_source_pos_per_label = collections.defaultdict(set)\n",
        "  f_out = open(output_tsv_path, 'w') if output_tsv_path else None\n",
        "  start_time = time.time()\n",
        "  for i, (base_i, source_i, inv_i) in enumerate(triplet_gen):\n",
        "    inv_example = examples[inv_i] if not is_form_only else inv_i\n",
        "    base_example = examples[base_i] if not is_form_only else base_i\n",
        "    inv_example = gen_inv_base_and_source_features(\n",
        "        base_example, examples[source_i], inv_example,\n",
        "        iit_config['max_num_inv'])\n",
        "    if not inv_example:\n",
        "      continue\n",
        "    unique_inv_label_per_base[inv_example['base_input']].add(inv_example['inv_label'])\n",
        "    if i % 10000 == 0:\n",
        "      print('Finished %d examples in %.2f sec.' % (i, time.time() - start_time))\n",
        "    if not f_out:\n",
        "      inv_examples.append(inv_example)\n",
        "      continue\n",
        "    if not inv_example:\n",
        "      continue\n",
        "    if (max([loc[0] for loc in inv_example['base_locations']]) >= max_input_seq_len) or (\n",
        "        max([loc[0] for loc in inv_example['source_locations']]) >= max_input_seq_len):\n",
        "      continue\n",
        "    # Change (token_index, char_pos) to (token_index, char_pos, char_pos+1) to mark the space of the char.\n",
        "    base_locs = [loc + [loc[-1] + 1 if loc[-1] >= 0 else -1] for loc in inv_example['base_locations']]\n",
        "    source_locs = [loc + [loc[-1] + 1 if loc[-1] >= 0 else -1] for loc in inv_example['source_locations']]\n",
        "    data = [f\"{inv_example['base_input']}\\t{inv_example['source_input']}\\t{inv_example['inv_label']}\"] + [\n",
        "            '\\t'.join(map(str, loc)) for loc in base_locs] + [\n",
        "            '\\t'.join(map(str, loc)) for loc in source_locs]\n",
        "    f_out.write('\\t'.join(data) + '\\n')\n",
        "  print('avg #inv_label_per_base=%.2f' % (np.mean(list(map(len, unique_inv_label_per_base.values())))),\n",
        "        'min #inv_label_per_base=%.2f' % (min(map(len, unique_inv_label_per_base.values()))),\n",
        "        'max #inv_label_per_base=%.2f' % (max(map(len, unique_inv_label_per_base.values()))))\n",
        "  if not output_tsv_path:\n",
        "    return inv_examples\n",
        "  f_out.close()\n",
        "  !wc -l $output_tsv_path\n",
        "  !head -n 5 $output_tsv_path\n",
        "  !tail -n 5 $output_tsv_path\n",
        "  return None\n",
        "\n",
        "\n",
        "def test_gen_inv_example():\n",
        "  split = 'train'\n",
        "  inv_examples = gen_inv_examples(anno_input_tsv_path, None, 50, task_name)\n",
        "  for inv_example in inv_examples:\n",
        "    if inv_example is None:\n",
        "      continue\n",
        "    _, base_input_tokens = get_pos_to_token_index(inv_example['base_input'])\n",
        "    _, source_input_tokens = get_pos_to_token_index(inv_example['source_input'])\n",
        "    print('BASE:', inv_example['base_input'], '\\t', inv_example['base_label'])\n",
        "    print('SOURCE:', inv_example['source_input'], '\\t', inv_example['source_label'])\n",
        "    print('INV:', inv_example['inv_input'], '\\t', inv_example['inv_label'])\n",
        "    print(base_input_tokens)\n",
        "    print(inv_example['base_locations'])\n",
        "    print([(base_input_tokens[s[0]], base_input_tokens[s[0]][s[1]:s[1] + 1])\n",
        "           for s in inv_example['base_locations'] if s[-1] != -1])\n",
        "    print(source_input_tokens)\n",
        "    print(inv_example['source_locations'])\n",
        "    print([(source_input_tokens[s[0]], source_input_tokens[s[0]][s[1]:s[1] + 1])\n",
        "           for s in inv_example['source_locations'] if s[-1] != -1])"
      ],
      "metadata": {
        "id": "a3hSYxFL2IzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders"
      ],
      "metadata": {
        "id": "hvtDOYEu81vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown TSV Data Loader\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def preproc_tokenize(examples, max_input_seq_len, max_output_seq_len,\n",
        "                     input_feature=None, label_feature=None,\n",
        "                     extra_feature_to_tokenize=None,\n",
        "                     source_tokenizer=None,  target_tokenizer=None):\n",
        "  source_tokenizer = source_tokenizer or t5_default_tokenizer\n",
        "  target_tokenizer = target_tokenizer or t5_default_tokenizer\n",
        "  input_batch = copy.deepcopy(examples)\n",
        "  input_feature = input_feature or 'input'\n",
        "  label_feature = label_feature or 'label'\n",
        "  input_batch.update(\n",
        "      source_tokenizer(examples[input_feature], padding=\"max_length\",\n",
        "                max_length=max_input_seq_len, return_tensors=\"pt\", truncation=True))\n",
        "  labels = target_tokenizer(examples[label_feature], padding=\"max_length\",\n",
        "                     max_length=max_output_seq_len, return_tensors=\"pt\", truncation=True).input_ids\n",
        "  labels[labels == target_tokenizer.pad_token_id] = -100\n",
        "  input_batch['labels'] = labels\n",
        "  if extra_feature_to_tokenize:\n",
        "    for feat in extra_feature_to_tokenize:\n",
        "      tokenized_feat = source_tokenizer(\n",
        "          examples[feat], padding=\"max_length\", max_length=max_input_seq_len,\n",
        "          return_tensors=\"pt\", truncation=True)\n",
        "      input_batch[f'{feat}_ids'] = tokenized_feat.input_ids\n",
        "      input_batch[f'{feat}_attention_mask'] = tokenized_feat.attention_mask\n",
        "  return input_batch\n",
        "\n",
        "\n",
        "def parse_tsv_line(line, feature_to_column):\n",
        "  parsed = line['text'].strip().split('\\t')\n",
        "  return {k: parsed[v] for k, v in feature_to_column.items()}\n",
        "\n",
        "\n",
        "def gen_seq2seq_dataset_from_tsv(split_to_files, feature_to_column, max_seq_len,\n",
        "                                 parse_fn=None, extra_feature_to_tokenize=None,\n",
        "                                 source_tokenizer=None, target_tokenizer=None):\n",
        "  max_input_seq_len, max_output_seq_len = max_seq_len\n",
        "  if parse_fn is None:\n",
        "    parse_fn = parse_tsv_line\n",
        "  dataset = load_dataset(\"text\", data_files=split_to_files)\n",
        "  dataset = dataset.map(lambda x: parse_fn(x, feature_to_column))\n",
        "  print(dataset)\n",
        "  # print examples\n",
        "  for split in dataset:\n",
        "    for i in range(3):\n",
        "      print('%s split example %d:' % (split.upper(), (i + 1)))\n",
        "      print('input: %s' % dataset[split]['input'][i])\n",
        "      print('output: %s' % dataset[split]['label'][i])\n",
        "  tokenized_datasets = dataset.map(\n",
        "      lambda examples: preproc_tokenize(\n",
        "          examples, max_input_seq_len, max_output_seq_len,\n",
        "          extra_feature_to_tokenize,\n",
        "          source_tokenizer=source_tokenizer,\n",
        "          target_tokenizer=target_tokenizer),\n",
        "      batched=True)\n",
        "  removed_text_columns = ['input', 'label', 'text'] + (extra_feature_to_tokenize or [])\n",
        "  tokenized_datasets = tokenized_datasets.remove_columns(removed_text_columns)\n",
        "  tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
        "  return tokenized_datasets\n",
        "\n",
        "\n",
        "def gen_seq2seq_text_dataset_from_tsv(dataset_split_to_path,\n",
        "                                      parse_fn,\n",
        "                                      parse_fn_args=None,\n",
        "                                      keep_in_memory=None):\n",
        "  \"\"\"Generate raw text dataset without tokenization.\"\"\"\n",
        "  dataset = load_dataset(\"text\", data_files=dataset_split_to_path,\n",
        "                         keep_in_memory=keep_in_memory)\n",
        "  if parse_fn_args:\n",
        "    dataset = dataset.map(lambda x: parse_fn(x, parse_fn_args))\n",
        "  else:\n",
        "    dataset = dataset.map(parse_fn)\n",
        "  print(dataset)\n",
        "  # print examples\n",
        "  for split in dataset:\n",
        "    input_keys = [k for k in dataset[split].features.keys() if 'input' in k]\n",
        "    label_keys = [k for k in dataset[split].features.keys() if 'label' in k]\n",
        "    if not input_keys or not label_keys:\n",
        "      continue\n",
        "    input_key = 'input' if 'input' in input_keys else input_keys[0]\n",
        "    label_key = 'label' if 'label' in label_keys else label_keys[0]\n",
        "    for i in range(3):\n",
        "      print('%s split example %d:' % (split.upper(), (i + 1)))\n",
        "      print('%s: %s' % (input_key, dataset[split][input_key][i]))\n",
        "      print('%s: %s' % (label_key, dataset[split][label_key][i]))\n",
        "  removed_text_columns = ['text']\n",
        "  dataset = dataset.remove_columns(removed_text_columns)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "tC-MAHaHLSya",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(task_name, feature_to_column=None,\n",
        "                  splits=None, dataset_split_to_path=None,\n",
        "                  source_tokenizer=None, target_tokenizer=None):\n",
        "  if not feature_to_column:\n",
        "    feature_to_column = {'input': 0, 'label': 1}\n",
        "  if not dataset_split_to_path:\n",
        "    splits = splits or ('train', 'val')\n",
        "    dataset_split_to_path = {k: TASK_TO_DATASETS[task_name][k] for k in splits}\n",
        "  max_seq_len = (TASK_TO_DATASETS[task_name]['seq_length']['max_src_token'],\n",
        "                 TASK_TO_DATASETS[task_name]['seq_length']['max_trg_token'])\n",
        "  datasets = gen_seq2seq_dataset_from_tsv(\n",
        "      dataset_split_to_path, feature_to_column, max_seq_len,\n",
        "      source_tokenizer=source_tokenizer, target_tokenizer=target_tokenizer)\n",
        "\n",
        "  for key in datasets:\n",
        "    print(datasets[key]['input_ids'][:2])\n",
        "    print(datasets[key]['labels'][:2])\n",
        "  return datasets, dataset_split_to_path"
      ],
      "metadata": {
        "id": "yL2aH9FnIAhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##@markdown IIT Examples TSV Data Loader\n",
        "\n",
        "def parse_tsv_line_with_inv_example(line, iit_config):\n",
        "  num_loc = ANNO_LEN * iit_config['max_num_inv']\n",
        "  parsed = line['text'].strip('\\n').split('\\t')\n",
        "  base_loc_offset = 3\n",
        "  source_loc_offset = base_loc_offset + num_loc\n",
        "  example = {'base_input': parsed[0],\n",
        "             'source_input': parsed[1],\n",
        "             'inv_label': parsed[2],\n",
        "             'base_locations': list(map(int, parsed[base_loc_offset: source_loc_offset])),\n",
        "             'source_locations': list(map(int, parsed[source_loc_offset: source_loc_offset + num_loc]))}\n",
        "  return example\n",
        "\n",
        "\n",
        "def preproc_inv_example_fn(inv_examples, max_seq_len, iit_config):\n",
        "  max_input_seq_len, max_output_seq_len = max_seq_len\n",
        "  target_tokenizer=iit_config['target_tokenizer'] if 'target_tokenizer' in iit_config else None\n",
        "  tokenized_batch = preproc_tokenize(\n",
        "      inv_examples, max_input_seq_len, max_output_seq_len,\n",
        "      input_feature='base_input', label_feature='inv_label',\n",
        "      extra_feature_to_tokenize=['source_input'],\n",
        "      source_tokenizer=None,  target_tokenizer=target_tokenizer)\n",
        "  # Store as 8-bit signed integers.\n",
        "  tokenized_batch['base_locations'] = torch.CharTensor(tokenized_batch['base_locations'])\n",
        "  tokenized_batch['source_locations'] = torch.CharTensor(tokenized_batch['source_locations'])\n",
        "  if 'inv_values' in tokenized_batch:\n",
        "    tokenized_batch['inv_values'] = torch.CharTensor(tokenized_batch['inv_values'])\n",
        "    tokenized_batch['inv_value_locations'] = torch.CharTensor(tokenized_batch['inv_value_locations'])\n",
        "  return tokenized_batch\n",
        "\n",
        "\n",
        "eval_inv_annotated_feature_keys = set([\n",
        "    'labels',\n",
        "    'base_locations', 'source_locations',\n",
        "    'source_input_ids', 'base_input_ids',\n",
        "    'base_attention_mask', 'source_attention_mask',\n",
        "    'inv_values', 'inv_value_locations'])\n",
        "\n",
        "\n",
        "def gen_iit_dataset_from_tsv(task_name):\n",
        "  dataset_split_to_path = {'train': TASK_TO_IIT_DATA[task_name]['iit_train']}\n",
        "  max_seq_len = (TASK_TO_DATASETS[task_name]['seq_length']['max_src_token'],\n",
        "                 TASK_TO_DATASETS[task_name]['seq_length']['max_trg_token'])\n",
        "  iit_config = TASK_TO_IIT_DATA[task_name]['iit_config']\n",
        "\n",
        "  datasets.config.IN_MEMORY_MAX_SIZE = 1024**3  # 1G\n",
        "  text_datasets = gen_seq2seq_text_dataset_from_tsv(\n",
        "      dataset_split_to_path,\n",
        "      parse_fn_args=iit_config,\n",
        "      parse_fn=parse_tsv_line_with_inv_example,\n",
        "      keep_in_memory=True)\n",
        "\n",
        "  tokenized_datasets = text_datasets.map(\n",
        "      lambda x: preproc_inv_example_fn(x, max_seq_len, iit_config), batched=True)\n",
        "  tokenized_datasets = tokenized_datasets.rename_columns(\n",
        "      {'input_ids': 'base_input_ids',\n",
        "       'attention_mask': 'base_attention_mask',\n",
        "       'source_input_attention_mask': 'source_attention_mask'})\n",
        "  all_features = tokenized_datasets[list(tokenized_datasets.keys())[0]].features\n",
        "  removed_text_columns = [\n",
        "      k for k in all_features if k not in eval_inv_annotated_feature_keys]\n",
        "  tokenized_datasets = tokenized_datasets.remove_columns(removed_text_columns)\n",
        "  tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
        "  print(tokenized_datasets)\n",
        "  # print examples\n",
        "  for split in tokenized_datasets:\n",
        "    for i in range(10):\n",
        "      print('%s split example %d:' % (split.upper(), (i + 1)))\n",
        "      print('base_input: %s' % tokenizer.decode(\n",
        "          tokenized_datasets[split]['base_input_ids'][i], skip_special_tokens=True))\n",
        "      print('source_input: %s' % tokenizer.decode(\n",
        "          tokenized_datasets[split]['source_input_ids'][i], skip_special_tokens=True))\n",
        "      print('labels: %s' % tokenizer.decode(\n",
        "          torch.maximum(torch.zeros(1), tokenized_datasets[split]['labels'][i]),\n",
        "          skip_special_tokens=True))\n",
        "      print('label tokens:', tokenized_datasets[split]['labels'][i].tolist())\n",
        "  return tokenized_datasets"
      ],
      "metadata": {
        "id": "72KYclLVr1tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Causal Abstraction"
      ],
      "metadata": {
        "id": "Jf5tdMCA5MUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown `t5stack_forward_pre_block`\n",
        "def t5stack_forward_pre_block(\n",
        "        t5stack,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "  # Model parallel\n",
        "  if t5stack.model_parallel:\n",
        "      torch.cuda.set_device(t5stack.first_device)\n",
        "      t5stack.embed_tokens = t5stack.embed_tokens.to(t5stack.first_device)\n",
        "  use_cache = use_cache if use_cache is not None else t5stack.config.use_cache\n",
        "  output_attentions = output_attentions if output_attentions is not None else t5stack.config.output_attentions\n",
        "  output_hidden_states = (\n",
        "      output_hidden_states if output_hidden_states is not None else t5stack.config.output_hidden_states\n",
        "  )\n",
        "  return_dict = return_dict if return_dict is not None else t5stack.config.use_return_dict\n",
        "\n",
        "  if input_ids is not None and inputs_embeds is not None:\n",
        "      err_msg_prefix = \"decoder_\" if t5stack.is_decoder else \"\"\n",
        "      raise ValueError(\n",
        "          f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n",
        "      )\n",
        "  elif input_ids is not None:\n",
        "      input_shape = input_ids.size()\n",
        "      input_ids = input_ids.view(-1, input_shape[-1])\n",
        "  elif inputs_embeds is not None:\n",
        "      input_shape = inputs_embeds.size()[:-1]\n",
        "  else:\n",
        "      err_msg_prefix = \"decoder_\" if t5stack.is_decoder else \"\"\n",
        "      raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n",
        "\n",
        "  if inputs_embeds is None:\n",
        "      assert t5stack.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n",
        "      inputs_embeds = t5stack.embed_tokens(input_ids)\n",
        "\n",
        "  batch_size, seq_length = input_shape\n",
        "\n",
        "  # required mask seq length can be calculated via length of past\n",
        "  mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n",
        "\n",
        "  if use_cache is True:\n",
        "      assert t5stack.is_decoder, f\"`use_cache` can only be set to `True` if {t5stack} is used as a decoder\"\n",
        "\n",
        "  if attention_mask is None:\n",
        "      attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n",
        "  if t5stack.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:\n",
        "      encoder_seq_length = encoder_hidden_states.shape[1]\n",
        "      encoder_attention_mask = torch.ones(\n",
        "          batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n",
        "      )\n",
        "\n",
        "  # initialize past_key_values with `None` if past does not exist\n",
        "  if past_key_values is None:\n",
        "      past_key_values = [None] * len(t5stack.block)\n",
        "\n",
        "  # We can provide a t5stack-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "  # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "  extended_attention_mask = t5stack.get_extended_attention_mask(attention_mask, input_shape)\n",
        "\n",
        "  # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "  # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "  if t5stack.is_decoder and encoder_hidden_states is not None:\n",
        "      encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "      encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "      if encoder_attention_mask is None:\n",
        "          encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device)\n",
        "      encoder_extended_attention_mask = t5stack.invert_attention_mask(encoder_attention_mask)\n",
        "  else:\n",
        "      encoder_extended_attention_mask = None\n",
        "\n",
        "  # Prepare head mask if needed\n",
        "  head_mask = t5stack.get_head_mask(head_mask, t5stack.config.num_layers)\n",
        "  cross_attn_head_mask = t5stack.get_head_mask(cross_attn_head_mask, t5stack.config.num_layers)\n",
        "  position_bias = None\n",
        "  encoder_decoder_position_bias = None\n",
        "\n",
        "  hidden_states = t5stack.dropout(inputs_embeds)\n",
        "\n",
        "  # Initialize accumulating variables.\n",
        "  all_hidden_states = () if output_hidden_states else None\n",
        "  present_key_value_states = () if use_cache else None\n",
        "  all_attentions = () if output_attentions else None\n",
        "  all_cross_attentions = () if (output_attentions and t5stack.is_decoder) else None\n",
        "\n",
        "  return {'hidden_states': hidden_states,\n",
        "          'encoder_hidden_states': encoder_hidden_states,\n",
        "          'encoder_attention_mask': encoder_attention_mask,\n",
        "          'head_mask': head_mask,\n",
        "          'cross_attn_head_mask': cross_attn_head_mask,\n",
        "          'past_key_values': past_key_values,\n",
        "          'position_bias': position_bias,\n",
        "          'encoder_decoder_position_bias': encoder_decoder_position_bias,\n",
        "          'extended_attention_mask': extended_attention_mask,\n",
        "          'encoder_extended_attention_mask': encoder_extended_attention_mask,\n",
        "          # Parsed parameters.\n",
        "          'use_cache': use_cache,\n",
        "          'output_hidden_states': output_hidden_states,\n",
        "          'output_attentions': output_attentions,\n",
        "          'return_dict': return_dict,\n",
        "          # Accumulating vars.\n",
        "          'all_hidden_states': all_hidden_states,\n",
        "          'present_key_value_states': present_key_value_states,\n",
        "          'all_attentions': all_attentions,\n",
        "          'all_cross_attentions': all_cross_attentions}\n",
        "# Test\n",
        "# model.eval()\n",
        "# pre_block_outputs = t5stack_forward_pre_block(\n",
        "#     model.encoder,\n",
        "#     input_ids=test_input_batch['input_ids'],\n",
        "#     attention_mask=test_input_batch['attention_mask'])\n",
        "# print(pre_block_outputs['hidden_states'].shape)\n",
        "# print(pre_block_outputs['hidden_states'][:1, :3, :5])"
      ],
      "metadata": {
        "id": "TfQLQsTvmiB1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown `t5stack_forward_single_layer_in_block`\n",
        "def t5stack_forward_single_layer_in_block(\n",
        "        t5stack,\n",
        "        layer_index,\n",
        "        hidden_states,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask,\n",
        "        head_mask,\n",
        "        cross_attn_head_mask,\n",
        "        past_key_values,\n",
        "        position_bias,\n",
        "        encoder_decoder_position_bias,\n",
        "        extended_attention_mask,\n",
        "        encoder_extended_attention_mask,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        all_hidden_states=None,\n",
        "        present_key_value_states=None,\n",
        "        all_attentions=None,\n",
        "        all_cross_attentions=None\n",
        "    ):\n",
        "  layer_module, past_key_value = t5stack.block[layer_index], past_key_values[layer_index]\n",
        "  layer_head_mask = head_mask[layer_index]\n",
        "  cross_attn_layer_head_mask = cross_attn_head_mask[layer_index]\n",
        "  # Model parallel\n",
        "  if t5stack.model_parallel:\n",
        "      torch.cuda.set_device(hidden_states.device)\n",
        "      # Ensure that attention_mask is always on the same device as hidden_states\n",
        "      if attention_mask is not None:\n",
        "          attention_mask = attention_mask.to(hidden_states.device)\n",
        "      if position_bias is not None:\n",
        "          position_bias = position_bias.to(hidden_states.device)\n",
        "      if encoder_hidden_states is not None:\n",
        "          encoder_hidden_states = encoder_hidden_states.to(hidden_states.device)\n",
        "      if encoder_extended_attention_mask is not None:\n",
        "          encoder_extended_attention_mask = encoder_extended_attention_mask.to(hidden_states.device)\n",
        "      if encoder_decoder_position_bias is not None:\n",
        "          encoder_decoder_position_bias = encoder_decoder_position_bias.to(hidden_states.device)\n",
        "      if layer_head_mask is not None:\n",
        "          layer_head_mask = layer_head_mask.to(hidden_states.device)\n",
        "      if cross_attn_layer_head_mask is not None:\n",
        "          cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(hidden_states.device)\n",
        "  # Update accumulating variables.\n",
        "  if output_hidden_states:\n",
        "    all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "  if t5stack.gradient_checkpointing and t5stack.training:\n",
        "      if use_cache:\n",
        "          logger.warning(\n",
        "              \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "          )\n",
        "          use_cache = False\n",
        "\n",
        "      def create_custom_forward(module):\n",
        "          def custom_forward(*inputs):\n",
        "              return tuple(module(*inputs, use_cache, output_attentions))\n",
        "\n",
        "          return custom_forward\n",
        "\n",
        "      layer_outputs = checkpoint(\n",
        "          create_custom_forward(layer_module),\n",
        "          hidden_states,\n",
        "          extended_attention_mask,\n",
        "          position_bias,\n",
        "          encoder_hidden_states,\n",
        "          encoder_extended_attention_mask,\n",
        "          encoder_decoder_position_bias,\n",
        "          layer_head_mask,\n",
        "          cross_attn_layer_head_mask,\n",
        "          None,  # past_key_value is always None with gradient checkpointing\n",
        "      )\n",
        "  else:\n",
        "      layer_outputs = layer_module(\n",
        "          hidden_states,\n",
        "          attention_mask=extended_attention_mask,\n",
        "          position_bias=position_bias,\n",
        "          encoder_hidden_states=encoder_hidden_states,\n",
        "          encoder_attention_mask=encoder_extended_attention_mask,\n",
        "          encoder_decoder_position_bias=encoder_decoder_position_bias,\n",
        "          layer_head_mask=layer_head_mask,\n",
        "          cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n",
        "          past_key_value=past_key_value,\n",
        "          use_cache=use_cache,\n",
        "          output_attentions=output_attentions,\n",
        "      )\n",
        "\n",
        "  # layer_outputs is a tuple with:\n",
        "  # hidden-states, key-value-states, (t5stack-attention position bias), (t5stack-attention weights), (cross-attention position bias), (cross-attention weights)\n",
        "  if use_cache is False:\n",
        "      layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n",
        "\n",
        "  hidden_states, present_key_value_state = layer_outputs[:2]\n",
        "\n",
        "  # We share the position biases between the layers - the first layer store them\n",
        "  # layer_outputs = hidden-states, key-value-states (t5stack-attention position bias), (t5stack-attention weights),\n",
        "  # (cross-attention position bias), (cross-attention weights)\n",
        "  position_bias = layer_outputs[2]\n",
        "  if t5stack.is_decoder and encoder_hidden_states is not None:\n",
        "      encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n",
        "\n",
        "  # Model Parallel: If it's the last layer for that device, put things on the next device\n",
        "  if t5stack.model_parallel:\n",
        "      for k, v in t5stack.device_map.items():\n",
        "          if i == v[-1] and \"cuda:\" + str(k) != t5stack.last_device:\n",
        "              hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
        "\n",
        "  # Update accumulating variables.\n",
        "  if use_cache:\n",
        "    present_key_value_states = present_key_value_states + (present_key_value_state,)\n",
        "  if output_attentions:\n",
        "    all_attentions = all_attentions + (layer_outputs[3],)\n",
        "    if t5stack.is_decoder:\n",
        "      all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n",
        "\n",
        "  return {'hidden_states': hidden_states,\n",
        "          'position_bias': position_bias,\n",
        "          'present_key_value_states': present_key_value_states,\n",
        "          'all_hidden_states': all_hidden_states,\n",
        "          'all_attentions': all_attentions,\n",
        "          'all_cross_attentions': all_cross_attentions,\n",
        "          'output_hidden_states': output_hidden_states,\n",
        "          'return_dict': return_dict,}\n",
        "\n",
        "# Test\n",
        "#pre_layer_outputs = pre_block_outputs.copy()\n",
        "#for i in range(4):\n",
        "#  layer_outputs = t5stack_forward_single_layer_in_block(\n",
        "#          model.encoder,\n",
        "#          i,\n",
        "#          **pre_layer_outputs)\n",
        "#  for k in pre_block_outputs:\n",
        "#    if k in layer_outputs:\n",
        "#      pre_layer_outputs[k] = layer_outputs[k]\n",
        "#print(pre_layer_outputs['hidden_states'].shape)\n",
        "#print(pre_layer_outputs['hidden_states'][:1, :3, :5])"
      ],
      "metadata": {
        "id": "It_1B4u0n-Cs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown `t5stack_forward_post_block`\n",
        "\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
        "\n",
        "def t5stack_forward_post_block(\n",
        "        t5stack,\n",
        "        hidden_states,\n",
        "        all_hidden_states=None,\n",
        "        all_attentions=None,\n",
        "        all_cross_attentions=None,\n",
        "        present_key_value_states=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "  hidden_states = t5stack.final_layer_norm(hidden_states)\n",
        "  hidden_states = t5stack.dropout(hidden_states)\n",
        "\n",
        "  # Add last layer\n",
        "  if output_hidden_states:\n",
        "      all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "  if not return_dict:\n",
        "      return tuple(\n",
        "          v\n",
        "          for v in [\n",
        "              hidden_states,\n",
        "              present_key_value_states,\n",
        "              all_hidden_states,\n",
        "              all_attentions,\n",
        "              all_cross_attentions,\n",
        "          ]\n",
        "          if v is not None\n",
        "      )\n",
        "  return BaseModelOutputWithPastAndCrossAttentions(\n",
        "      last_hidden_state=hidden_states,\n",
        "      past_key_values=present_key_value_states,\n",
        "      hidden_states=all_hidden_states,\n",
        "      attentions=all_attentions,\n",
        "      cross_attentions=all_cross_attentions,\n",
        "  )\n",
        "\n",
        "\n",
        "# Test\n",
        "#block_outputs = t5stack_forward_post_block(\n",
        "#        model.encoder,\n",
        "#        **layer_outputs)\n",
        "#\n",
        "#print(block_outputs[0].shape)\n",
        "#print(block_outputs[0][:1, :3, :5])"
      ],
      "metadata": {
        "id": "bhIBzyS1FxSy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown encoder causal abstraction\n",
        "# interchange intervention: [batch, layer, step, pos_start, pos_end]\n",
        "# setting values: [batch, layer, step, pos_start, pos_end], index to some external embedding\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "from transformers.models.t5.modeling_t5 import T5Stack, T5ForConditionalGeneration\n",
        "\n",
        "# Need to override the encoder, as the generate call directly invoke the foward\n",
        "# function of the encoder to compute encoder output.\n",
        "# https://github.com/huggingface/transformers/blob/bc21aaca789f1a366c05e8b5e111632944886393/src/transformers/generation_utils.py#L506\n",
        "\n",
        "\n",
        "class TransformerEncoderCausalAbstraction(T5Stack):\n",
        "  def __init__(self, transformer_encoder):\n",
        "      super().__init__(transformer_encoder.config)\n",
        "      # Store a copy of pretrained Encoder.\n",
        "      self.encoder = transformer_encoder\n",
        "      # Store interventions.\n",
        "      self.encoder_inv_locations_to_values = None\n",
        "      # Copy over all attributes.\n",
        "      self.embed_tokens = transformer_encoder.embed_tokens\n",
        "      self.is_decoder = transformer_encoder.config.is_decoder\n",
        "      self.block = transformer_encoder.block\n",
        "      self.final_layer_norm = transformer_encoder.final_layer_norm\n",
        "      self.dropout = transformer_encoder.dropout\n",
        "      self.model_parallel = transformer_encoder.model_parallel\n",
        "      self.device_map = transformer_encoder.device_map\n",
        "      self.gradient_checkpointing = transformer_encoder.gradient_checkpointing\n",
        "\n",
        "  def get_hidden_states(self, input_ids, attention_mask, locations, partial_only=True):\n",
        "    \"\"\"GetVals\n",
        "\n",
        "      partial_only: If True, only run part of the Encoder upto the layer\n",
        "                    requested in the locations.\n",
        "    \"\"\"\n",
        "    # Encoder is a T5Stack (https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/t5/modeling_t5.py#L899)\n",
        "    if partial_only:\n",
        "      hidden_states = []\n",
        "      max_layer = max(loc[1] for loc in locations)\n",
        "      prev_layer_outputs = t5stack_forward_pre_block(\n",
        "          self.encoder,\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          inputs_embeds=None,\n",
        "          head_mask=None,\n",
        "          use_cache=None,\n",
        "          output_hidden_states=True,\n",
        "          output_attentions=None,\n",
        "          return_dict=None)\n",
        "      hidden_states.append(prev_layer_outputs['hidden_states'])\n",
        "      # Run block, i.e. transformer layers.\n",
        "      for layer_index in range(min(len(self.encoder.block), max_layer)):\n",
        "        layer_outputs = t5stack_forward_single_layer_in_block(\n",
        "                self.encoder, layer_index, **prev_layer_outputs)\n",
        "        for k in prev_layer_outputs:\n",
        "          if k in layer_outputs:\n",
        "            prev_layer_outputs[k] = layer_outputs[k]\n",
        "        hidden_states.append(prev_layer_outputs['hidden_states'])\n",
        "      if max_layer > len(self.encoder.block):\n",
        "        # Run post-block.\n",
        "        block_outputs = t5stack_forward_post_block(\n",
        "            self.encoder,\n",
        "            **layer_outputs)\n",
        "        hidden_states.append(block_outputs['hidden_states'])\n",
        "    else:\n",
        "      # Run full Encoder.\n",
        "      outputs = self.encoder(input_ids=input_ids,\n",
        "                             attention_mask=attention_mask,\n",
        "                             output_hidden_states=True)\n",
        "      # There are num_layer+1 hidden_states\n",
        "      # The last layer is the normalized output\n",
        "      # https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/t5/modeling_t5.py#L1087\n",
        "      # hidden_states: (num_layer + 1) * B * input_seq_len * dimension\n",
        "      hidden_states = outputs['hidden_states']\n",
        "    return {loc: hidden_states[loc[1]][loc[0], loc[2], :] if len(loc) == 3 else\n",
        "                 hidden_states[loc[1]][loc[0], loc[2], loc[3]:loc[4]]\n",
        "            for loc in locations}\n",
        "\n",
        "  def reset_interchange_interventions(self):\n",
        "    self.encoder_inv_locations_to_values = None\n",
        "\n",
        "  def set_interchange_interventions(\n",
        "      self, inv_input_ids, inv_attention_mask, locations, base_locations=None,\n",
        "      inv_values=None, inv_value_locations=None):\n",
        "    \"\"\"Set intervention parameters.\n",
        "\n",
        "    There are two ways to provide intervention values:\n",
        "      1) by setting source locations through locations\n",
        "      2) by directly providing values through inv_values\n",
        "\n",
        "    inv_input_ids: source input ids\n",
        "    inv_attention_mask: source attention mask\n",
        "    locations: locations to retrieve the source values of the internal variables\n",
        "    base_locations: locations to swap the values of the internal variables with\n",
        "                    source input. Default to the same locations as source.\n",
        "                    If set, must have the same shape as source locations.\n",
        "    inv_values: intervention values. If specified, inv_input_ids and\n",
        "                inv_attention_mask will be ignored.\n",
        "    inv_value_locations: locations to set to the intervention values.\n",
        "    \"\"\"\n",
        "    assert locations is not None or inv_values is not None\n",
        "    if locations is not None:\n",
        "      assert len(locations) == len(base_locations)\n",
        "    if inv_values is not None:\n",
        "      assert len(inv_values) == len(inv_value_locations)\n",
        "    self.encoder_inv_locations_to_values = (\n",
        "        inv_input_ids, inv_attention_mask, locations, base_locations,\n",
        "        inv_values, inv_value_locations)\n",
        "\n",
        "  def get_interchange_interventions(self):\n",
        "    inv_loc_to_values = {}\n",
        "    if self.encoder_inv_locations_to_values is not None:\n",
        "      (inv_input_ids, inv_attention_mask, source_locations, _, inv_values, _\n",
        "          ) = self.encoder_inv_locations_to_values\n",
        "      if source_locations is not None:\n",
        "        inv_loc_to_values.update(\n",
        "            self.get_hidden_states(inv_input_ids, inv_attention_mask, source_locations))\n",
        "      if inv_values is not None:\n",
        "        inv_loc_to_values.update({i: inv_values[i] for i in range(len(inv_values))})\n",
        "    return inv_loc_to_values\n",
        "\n",
        "  def forward_with_intervention(\n",
        "      self, input_ids, attention_mask,\n",
        "      inv_locations_to_values,\n",
        "      encoder_hidden_states=None,\n",
        "      encoder_attention_mask=None,\n",
        "      inputs_embeds=None,\n",
        "      head_mask=None,\n",
        "      cross_attn_head_mask=None,\n",
        "      past_key_values=None,\n",
        "      use_cache=None, output_hidden_states=None,\n",
        "      output_attentions=None, return_dict=None):\n",
        "    \"\"\"IntInv\n",
        "    inv_locations_to_values: A dict of\n",
        "      (batch_index, layer_index, step_index): Tensor(float32) or\n",
        "      (batch_index, layer_index, step_index, dim_begin, dim_end): Tensor(float32)\n",
        "      indicating the location of the representations to interchange with\n",
        "      the intervention values.\n",
        "    \"\"\"\n",
        "    # sort inv_locations by layers.\n",
        "    sorted_loc = sorted(inv_locations_to_values)\n",
        "    # Run pre-block, i.e. process inputs and embedding layers.\n",
        "    prev_layer_outputs = t5stack_forward_pre_block(\n",
        "      self.encoder,\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      inputs_embeds=inputs_embeds,\n",
        "      head_mask=head_mask,\n",
        "      use_cache=use_cache,\n",
        "      output_hidden_states=output_hidden_states,\n",
        "      output_attentions=output_attentions,\n",
        "      return_dict=return_dict)\n",
        "    # Run block, i.e. transformer layers.\n",
        "    for layer_index in range(len(self.encoder.block)):\n",
        "      # Apply intervention on layer i step t BEFORE the hidden state update,\n",
        "      # as the first set of hidden state returned by the model are the input\n",
        "      # embeddings, not the outputs of the first layer.\n",
        "      for loc in sorted_loc:\n",
        "        b_i, l_i, s_i = loc[:3]\n",
        "        if l_i > layer_index:\n",
        "          break\n",
        "        if l_i == layer_index:\n",
        "          if len(loc) == 3:\n",
        "            prev_layer_outputs['hidden_states'][b_i, s_i, :] = (\n",
        "                inv_locations_to_values[(b_i, l_i, s_i)])\n",
        "          else:\n",
        "            prev_layer_outputs['hidden_states'][b_i, s_i, loc[3]:loc[4]] = (\n",
        "                inv_locations_to_values[(b_i, l_i, s_i, loc[3], loc[4])])\n",
        "      layer_outputs = t5stack_forward_single_layer_in_block(\n",
        "              self.encoder, layer_index, **prev_layer_outputs)\n",
        "      for k in prev_layer_outputs:\n",
        "        if k in layer_outputs:\n",
        "          prev_layer_outputs[k] = layer_outputs[k]\n",
        "    # Run post-block.\n",
        "    block_outputs = t5stack_forward_post_block(\n",
        "        self.encoder,\n",
        "        **layer_outputs)\n",
        "    # Apply intervention on the last layer output, which is past the layer norm.\n",
        "    for loc in sorted_loc:\n",
        "      b_i, l_i, s_i = loc[:3]\n",
        "      if l_i == len(self.encoder.block):\n",
        "        if len(loc) == 3:\n",
        "          block_outputs['last_hidden_state'][b_i, s_i, :] = inv_locations_to_values[(b_i, l_i, s_i)]\n",
        "        else:\n",
        "          #print(f'INV at {(b_i, l_i, s_i, loc[3], loc[4])}')\n",
        "          #print(block_outputs['last_hidden_state'][b_i, s_i, loc[3]:loc[4]])\n",
        "          block_outputs['last_hidden_state'][b_i, s_i, loc[3]:loc[4]] = (\n",
        "              inv_locations_to_values[(b_i, l_i, s_i, loc[3], loc[4])])\n",
        "          #print(inv_locations_to_values[(b_i, l_i, s_i, loc[3], loc[4])])\n",
        "    # Update all hidden_states after updating the last_hidden_state.\n",
        "    if 'hidden_states' in block_outputs:\n",
        "      block_outputs['hidden_states'] = (\n",
        "          block_outputs['hidden_states'][:-1] + (block_outputs['last_hidden_state'],))\n",
        "    return block_outputs\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      input_ids=None,\n",
        "      attention_mask=None,\n",
        "      encoder_hidden_states=None,\n",
        "      encoder_attention_mask=None,\n",
        "      inputs_embeds=None,\n",
        "      head_mask=None,\n",
        "      cross_attn_head_mask=None,\n",
        "      past_key_values=None,\n",
        "      use_cache=None,\n",
        "      output_attentions=None,\n",
        "      output_hidden_states=None,\n",
        "      return_dict=None):\n",
        "    # The values are indexed by source_locations or index.\n",
        "    source_locations_to_values = self.get_interchange_interventions()\n",
        "    base_locations_to_values = {}\n",
        "    # Update intervention locations if base and source use different sets of\n",
        "    # locations.\n",
        "    if source_locations_to_values:\n",
        "      (_, _, source_locations, base_locations, _, inv_value_locations\n",
        "       ) = self.encoder_inv_locations_to_values\n",
        "      if base_locations is not None:\n",
        "        base_locations_to_values.update({\n",
        "            base_locations[i]: source_locations_to_values[source_locations[i]]\n",
        "            for i in range(len(base_locations))})\n",
        "      if inv_value_locations is not None:\n",
        "        base_locations_to_values.update({\n",
        "            inv_value_locations[i]: source_locations_to_values[i]\n",
        "            for i in range(len(inv_value_locations))})\n",
        "      # Set base and source with the same locations.\n",
        "      if base_locations is None and inv_value_locations is None:\n",
        "        base_locations_to_values.update(source_locations_to_values)\n",
        "    return self.forward_with_intervention(\n",
        "          inv_locations_to_values=base_locations_to_values,\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          encoder_hidden_states=encoder_hidden_states,\n",
        "          encoder_attention_mask=encoder_attention_mask,\n",
        "          inputs_embeds=inputs_embeds,\n",
        "          head_mask=head_mask,\n",
        "          cross_attn_head_mask=cross_attn_head_mask,\n",
        "          past_key_values=past_key_values,\n",
        "          use_cache=use_cache,\n",
        "          output_attentions=output_attentions,\n",
        "          output_hidden_states=output_hidden_states,\n",
        "          return_dict=return_dict,\n",
        "      )\n",
        "\n",
        "class TransformerCausalAbstraction(T5ForConditionalGeneration):\n",
        "  def __init__(self, transformer):\n",
        "      \"\"\"\n",
        "      Causal abstraction of Transformer models.\n",
        "      \"\"\"\n",
        "      super().__init__(transformer.config)\n",
        "      # Store a copy of the pretrained transformer.\n",
        "      self.transformer = transformer\n",
        "      # Copy over all the attributes.\n",
        "      self.model_dim = self.transformer.model_dim\n",
        "      self.shared = self.transformer.shared\n",
        "      self.encoder = TransformerEncoderCausalAbstraction(\n",
        "          self.transformer.encoder)\n",
        "      self.decoder = self.transformer.decoder\n",
        "      self.lm_head = self.transformer.lm_head\n",
        "      self.model_parallel = self.transformer.model_parallel\n",
        "      self.device_map = self.transformer.device_map\n",
        "\n",
        "  def get_encoder(self):\n",
        "    return self.encoder"
      ],
      "metadata": {
        "id": "s8Hinzp-bsPx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Utils"
      ],
      "metadata": {
        "id": "6pK4s0V68fYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title IIT train step\n",
        "\n",
        "def add_batch_layer_dim_location_map_fn(locations, iit_layer, feat_dim):\n",
        "  # B * NUM_INV * ANNO_LEN\n",
        "  locations = locations.view(locations.shape[0], -1, ANNO_LEN).int()\n",
        "  locations[:, :, -2:] *= feat_dim\n",
        "  locations = locations.tolist()\n",
        "  locations = [tuple([b_i, iit_layer] + loc)\n",
        "          # Flatten the locations\n",
        "          for b_i in range(len(locations))\n",
        "          for loc in locations[b_i] if loc[-1] >= 0]\n",
        "  return locations\n",
        "\n",
        "def iit_train_step(causal_abstraction, input_batch, iit_input_batch_list,\n",
        "                   location_map_fn, base_factor=1.0, inv_factor=1.0):\n",
        "  \"\"\"IIT training step.\n",
        "\n",
        "    location_map_fn: batch of location in input => mapped location in model representations\n",
        "  \"\"\"\n",
        "  if isinstance(iit_input_batch_list, dict):\n",
        "    iit_input_batch_list = [iit_input_batch_list]\n",
        "  # base training\n",
        "  causal_abstraction.encoder.reset_interchange_interventions()\n",
        "  outputs = causal_abstraction(**input_batch)\n",
        "  loss = base_factor * outputs.loss\n",
        "  inv_outputs = None\n",
        "  # iit training\n",
        "  if iit_input_batch_list:\n",
        "    iit_batch_size = len(iit_input_batch_list[0][\"source_input_ids\"])\n",
        "    # For extracting intervention values from the source.\n",
        "    iit_base_input_ids, iit_base_attention_mask, iit_labels = [], [], []\n",
        "    iit_source_input_ids, iit_source_attention_mask = [], []\n",
        "    iit_base_localtions, iit_source_locations = [], []\n",
        "    # For directly provided intervention values.\n",
        "    iit_inv_values, iit_inv_value_locations = [], []\n",
        "    for i in range(len(iit_input_batch_list)):\n",
        "      iit_source_input_ids.append(iit_input_batch_list[i][\"source_input_ids\"])\n",
        "      iit_source_attention_mask.append(iit_input_batch_list[i][\"source_attention_mask\"])\n",
        "      base_locations = location_map_fn(iit_input_batch_list[i][\"base_locations\"])\n",
        "      source_locations = location_map_fn(iit_input_batch_list[i][\"source_locations\"])\n",
        "      iit_base_localtions.extend(base_locations)\n",
        "      iit_source_locations.extend(source_locations)\n",
        "      iit_base_input_ids.append(iit_input_batch_list[i][\"base_input_ids\"])\n",
        "      iit_base_attention_mask.append(iit_input_batch_list[i][\"base_attention_mask\"])\n",
        "      iit_labels.append(iit_input_batch_list[i][\"labels\"])\n",
        "    iit_base_input_ids = torch.cat(iit_base_input_ids, axis=0).to(device)\n",
        "    iit_base_attention_mask = torch.cat(iit_base_attention_mask, axis=0).to(device)\n",
        "    iit_labels = torch.cat(iit_labels, axis=0).to(device)\n",
        "    inv_outputs = causal_abstraction(\n",
        "        input_ids=iit_base_input_ids,\n",
        "        attention_mask=iit_base_attention_mask,\n",
        "        labels=iit_labels)\n",
        "    causal_abstraction.encoder.reset_interchange_interventions()\n",
        "    loss += inv_factor * inv_outputs.loss\n",
        "  return loss, outputs, inv_outputs"
      ],
      "metadata": {
        "id": "BN_k9m3pD2XX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Training metrics\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "eval_loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  # accuracy\n",
        "  outputs, labels = eval_pred\n",
        "  # outputs = [logits, hidden_states]\n",
        "  logits = outputs[0]\n",
        "  # numpy array\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  mask = ((labels != -100) & (labels != 1) & (labels != 0))\n",
        "  mask = mask.astype(float)\n",
        "  # loss\n",
        "  loss = eval_loss_fct(\n",
        "      torch.from_numpy(logits.reshape(-1, logits.shape[-1])).to(device),\n",
        "      torch.from_numpy(labels.reshape(-1)).to(device)).mean()\n",
        "  loss = float(loss.detach().cpu().numpy())\n",
        "  return {\n",
        "        'token_accuracy': float(\n",
        "            ((predictions == labels).astype(float) * mask).sum() / max(1, mask.sum())),\n",
        "        'sequence_accuracy': (\n",
        "            ((predictions == labels) | ~mask.astype(bool)).all(axis=-1).astype(float).sum() / len(mask)),\n",
        "        'loss': loss}"
      ],
      "metadata": {
        "id": "W1K_B_iWUx4Z",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Other training utils\n",
        "\n",
        "def freeze_encoder_first_n_layers(model, n):\n",
        "  for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "  # Unfreeze from n to end\n",
        "  for i in range(n, len(model.encoder.block)):\n",
        "    for param in model.encoder.block[i].parameters():\n",
        "      param.requires_grad = True\n",
        "  for param in model.encoder.final_layer_norm.parameters():\n",
        "    param.requires_grad = True\n",
        "  return model\n",
        "\n",
        "def train_step(model, input_batch):\n",
        "  outputs = model(**input_batch)\n",
        "  return outputs.loss, outputs\n",
        "\n",
        "def eval_step(model, input_batch):\n",
        "  outputs = model(**input_batch)\n",
        "  return outputs.loss, outputs\n",
        "\n",
        "def run_eval(model, eval_dataloader, first_n_batch=None, metric_prefix=None):\n",
        "  if metric_prefix is None:\n",
        "    metric_prefix = ''\n",
        "  model.eval()\n",
        "  eval_metrics = collections.defaultdict(list)\n",
        "  for i, eval_input_batch in enumerate(eval_dataloader):\n",
        "    if first_n_batch is not None and i > first_n_batch:\n",
        "      break\n",
        "    for k in eval_input_batch:\n",
        "      eval_input_batch[k] = eval_input_batch[k].to(device)\n",
        "    _, outputs = eval_step(model, eval_input_batch)\n",
        "    metrics = compute_metrics(([outputs.logits.detach().cpu().numpy()],\n",
        "                                eval_input_batch['labels'].cpu().numpy()))\n",
        "    for key in metrics:\n",
        "      eval_metrics[key].append(metrics[key])\n",
        "  for key in eval_metrics:\n",
        "    eval_metrics[metric_prefix + key] = float(np.array(eval_metrics[key]).mean())\n",
        "  return eval_metrics"
      ],
      "metadata": {
        "id": "2mKvfr9BPSeH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Utils"
      ],
      "metadata": {
        "id": "txRXSIlH8jG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Evaluation metrics\n",
        "\n",
        "import collections\n",
        "import json\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "ENGLISH_WORDS_30K = json.load(open(os.path.join(DATA_DIR, 'english_words_30k.json')))\n",
        "ENGLISH_WORDS_200K = json.load(open(os.path.join(DATA_DIR, 'english_words_200k.json')))\n",
        "ANAGRAM_DICT = json.load(open(os.path.join(DATA_DIR, 'anagrams_from_200k.json')))\n",
        "AMBIGUOUS_TYPOS = json.load(open(os.path.join(DATA_DIR, 'ambiguous_typos_18k.json')))\n",
        "\n",
        "def print_stats(eval_outputs):\n",
        "  stats, metrics = eval_outputs\n",
        "  total = stats['count']\n",
        "  print('String-level accuracy %.2f%%' % (100 * stats['match_fullstring'] / total))\n",
        "  print('Token-level accuracy  %.2f%%' % (100 * np.mean(metrics['token_accuracy'])))\n",
        "\n",
        "  print('\\nRelaxed matching:')\n",
        "  for key, val in sorted(stats.items()):\n",
        "    if key.startswith('match_') and not key.startswith('match_fullstring'):\n",
        "      print('%s +%.2f%%' % (key, 100 * val / total))\n",
        "\n",
        "\n",
        "def compute_matching_stats(input_text, target_text, output_text):\n",
        "  def space_normalize(text):\n",
        "    return text.strip().replace(' ', '')\n",
        "  input_norm = space_normalize(input_text)\n",
        "  output_norm = space_normalize(output_text)\n",
        "  target_norm = space_normalize(target_text)\n",
        "  stats = {}\n",
        "  if len(output_text) < 20:\n",
        "    stats['match_dictionary_30k'] = int(output_norm.lower() in ENGLISH_WORDS_30K)\n",
        "    stats['match_dictionary_200k'] = int(output_norm.lower() in ENGLISH_WORDS_200K)\n",
        "  if output_text.strip() == target_text.strip():\n",
        "    stats['match_fullstring'] = 1\n",
        "  elif input_text in AMBIGUOUS_TYPOS and output_text in AMBIGUOUS_TYPOS[input_text]:\n",
        "    stats['match_spelling_correction'] = 1\n",
        "  elif output_text in target_text or target_text in output_text:\n",
        "    stats['match_target_substring'] = 1\n",
        "  elif output_norm in input_norm:\n",
        "    stats['match_input_substring'] = 1\n",
        "  elif sorted(output_norm) == sorted(target_norm):\n",
        "    valid = 'valid' if output_norm in ANAGRAM_DICT else 'invalid'\n",
        "    stats[f'match_anagram_{valid}'] = 1\n",
        "  elif set(output_text) == set(target_text):\n",
        "    stats['match_char_set'] = 1\n",
        "  return stats\n",
        "\n",
        "\n",
        "def eval_topk_accuracy(model, test_dataset, max_output_seq_len,\n",
        "                       first_n=None, beam_size=None, batch_size=128,\n",
        "                       target_tokenizer=None):\n",
        "  target_tokenizer = target_tokenizer or t5_default_tokenizer\n",
        "  if isinstance(test_dataset, dict):\n",
        "    test_dataset = Dataset.from_dict(test_dataset).with_format(\"torch\")\n",
        "  eval_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "  model.eval()\n",
        "  stats = collections.defaultdict(int)\n",
        "  metrics = collections.defaultdict(list)\n",
        "  inputs_to_outputs = {}\n",
        "  for b_i, batch in enumerate(eval_dataloader):\n",
        "    if first_n and b_i >= first_n:\n",
        "      break\n",
        "    if b_i % 10 == 0:\n",
        "      print(f'Finished {b_i} batch')\n",
        "    for k in batch:\n",
        "      batch[k] = batch[k].to(device)\n",
        "    input_texts = target_tokenizer.batch_decode(\n",
        "      batch['input_ids'], skip_special_tokens=True)\n",
        "    target_texts = target_tokenizer.batch_decode(\n",
        "      torch.maximum(batch['labels'], torch.zeros_like(batch['labels'])),\n",
        "      skip_special_tokens=True)\n",
        "    with torch.no_grad():\n",
        "      predictions = model(**batch)\n",
        "      if beam_size:\n",
        "        outputs = model.generate(input_ids=batch['input_ids'],\n",
        "                                 attention_mask=batch['attention_mask'],\n",
        "                                 num_beams=beam_size,\n",
        "                                 num_return_sequences=beam_size,\n",
        "                                 do_sample=False,\n",
        "                                 max_length=max_output_seq_len,\n",
        "                                 length_penalty=0.05)  # (B*K, MAX_OUTPUT_SEQ_LEN)\n",
        "      else:\n",
        "        beam_size = 1\n",
        "        outputs = model.generate(input_ids=batch['input_ids'],\n",
        "                                 attention_mask=batch['attention_mask'],\n",
        "                                 max_length=max_output_seq_len)\n",
        "      output_texts = target_tokenizer.batch_decode(\n",
        "          outputs, skip_special_tokens=True)\n",
        "      for i in range(len(input_texts)):\n",
        "        output_text = output_texts[i * beam_size]\n",
        "        inputs_to_outputs[(input_texts[i], target_texts[i])] = output_text\n",
        "        stats['count'] += 1\n",
        "        matching_stats = compute_matching_stats(\n",
        "            input_texts[i], target_texts[i], output_text)\n",
        "        for k, v in matching_stats.items():\n",
        "          stats[k] += v\n",
        "        # Log the first few examples\n",
        "        if b_i < 10 and i == 0:\n",
        "          print('Input: %s' % input_texts[i])\n",
        "          print('Label: %s' % target_texts[i])\n",
        "          print('Pred:  %s' % output_text)\n",
        "    batch_metrics = compute_metrics(([predictions.logits.detach().cpu().numpy()],\n",
        "                                     batch['labels'].detach().cpu().numpy()))\n",
        "    for k, v in batch_metrics.items():\n",
        "      metrics[k].append(v)\n",
        "  print_stats((stats, metrics))\n",
        "  return (stats, metrics), inputs_to_outputs"
      ],
      "metadata": {
        "id": "rD4l9JJLmV3l",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Extract character representations\n",
        "\n",
        "import string\n",
        "\n",
        "def parse_annotated_examples(anno_tsv_path, first_n_line, parse_fn, parse_fn_args):\n",
        "  with open(anno_tsv_path, 'r') as f:\n",
        "    lines = f.readlines()[:first_n_line]\n",
        "    examples = list(map(lambda x: parse_fn(x, parse_fn_args), lines))\n",
        "  return examples\n",
        "\n",
        "def extract_char_representations(model, anno_tsv_path, data_config,\n",
        "                                 first_n_line=2048,\n",
        "                                 positions=None, iit_layer=1, char_dim=16,\n",
        "                                 eval_batch_size = 128,\n",
        "                                 source_tokenizer=None):\n",
        "  if not source_tokenizer:\n",
        "    source_tokenizer = t5_default_tokenizer\n",
        "  # parse the annotations\n",
        "  examples = parse_annotated_examples(anno_tsv_path, first_n_line=first_n_line,\n",
        "                                      parse_fn=parse_anno_line,\n",
        "                                      parse_fn_args=data_config)\n",
        "  # return a dict of char, position to list of vectors.\n",
        "  model.eval()\n",
        "  char_to_vec = collections.defaultdict(lambda: collections.defaultdict(list))\n",
        "  # Avoid having the same token multiple times.\n",
        "  dedup_keys = {}\n",
        "  for step in range(first_n_line // eval_batch_size):\n",
        "    input_text = [example['input'] for example in examples[step * eval_batch_size: (step + 1) * eval_batch_size]]\n",
        "    input_batch = source_tokenizer(\n",
        "        input_text, return_tensors=\"pt\", padding=\"max_length\",\n",
        "        max_length=data_config['max_input_seq_len'], truncation=True)\n",
        "    for key in input_batch:\n",
        "      input_batch[key] = input_batch[key].to(device)\n",
        "    with torch.no_grad():\n",
        "      enc_outputs = model.encoder(**input_batch, output_hidden_states=True)\n",
        "      for b_i in range(len(input_text)):\n",
        "        for loc in examples[step * eval_batch_size + b_i]['inv_locs']:\n",
        "          if loc[-1] < 0:\n",
        "            continue\n",
        "          if positions and loc[1] not in positions:\n",
        "            continue\n",
        "          tid = int(input_batch['input_ids'][b_i][loc[0]])\n",
        "          c = VOCAB[tid][loc[1]: loc[2]]\n",
        "          if (tid, loc[1]) in dedup_keys:\n",
        "            continue\n",
        "          dedup_keys[(tid, loc[1])] = True\n",
        "          char_to_vec[c][loc[1]].append(\n",
        "              enc_outputs['hidden_states'][iit_layer][\n",
        "                  b_i, loc[0], loc[1] * char_dim : loc[2] * char_dim].detach().cpu().numpy())\n",
        "  return {k: dict(v) for k, v in char_to_vec.items()}"
      ],
      "metadata": {
        "id": "uRBfGV37SAzy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Visualization of char representations\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "char_colors = [\n",
        "    ('bisque', '#FFE4C4'), # a\n",
        "    ('blue', '#0000FF'),\n",
        "    ('blueviolet', '#8A2BE2'),\n",
        "    ('brown', '#A52A2A'),\n",
        "    ('burlywood', '#DEB887'), # e\n",
        "    ('cadetblue', '#5F9EA0'),\n",
        "    ('chartreuse', '#7FFF00'),\n",
        "    ('chocolate', '#D2691E'),\n",
        "    ('coral', '#FF7F50'), # i\n",
        "    ('cornflowerblue', '#6495ED'),\n",
        "    ('crimson', '#DC143C'),\n",
        "    ('cyan', '#00FFFF'),\n",
        "    ('deepskyblue', '#00BFFF'),\n",
        "    ('darkgoldenrod', '#B8860B'),\n",
        "    ('darkorange', '#FF8C00'), # o\n",
        "    ('darkgreen', '#006400'),\n",
        "    ('darkmagenta', '#8B008B'),\n",
        "    ('darkolivegreen', '#556B2F'),\n",
        "    ('darkorchid', '#9932CC'),\n",
        "    ('darksalmon', '#E9967A'),\n",
        "    ('gold', '#FFD700'), # u\n",
        "    ('darkseagreen', '#8FBC8F'),\n",
        "    ('darkslateblue', '#483D8B'),\n",
        "    ('forestgreen', '#228B22'),\n",
        "    ('deeppink', '#FF1493'),\n",
        "    ('goldenrod', '#DAA520'),]\n",
        "\n",
        "\n",
        "def lighter_color(c):\n",
        "  rgb = sum([int(128 + eval(f'0x{c[i:i+2]}') * 0.5) << (4 * (5 - i))\n",
        "             for i in range(1,6,2)])\n",
        "  return f'{rgb:#08x}'.strip('-').replace('0x', '#')\n",
        "\n",
        "char_colors = char_colors + [\n",
        "    (f'light_{c[0]}', lighter_color(c[1])) for c in char_colors]\n",
        "\n",
        "\n",
        "# PCA & cluster\n",
        "def visualize_pca_2d(char_to_vec, task_name):\n",
        "  labels = [(k, p) for k in char_to_vec for p in char_to_vec[k]\n",
        "            for _ in range(len(char_to_vec[k][p]))]\n",
        "  X = np.concatenate([np.stack(char_to_vec[k][p], axis=0)\n",
        "            for k in char_to_vec for p in char_to_vec[k]], axis=0)\n",
        "  print(f'PCA over {len(labels)} vectors.')\n",
        "  X = (X - np.mean(X, axis=0, keepdims=True)) / X.var(axis=0)**0.5\n",
        "  pca = PCA(n_components=2)\n",
        "  pca.fit(X)\n",
        "  print(f'explained_variance_ratio={pca.explained_variance_ratio_}')\n",
        "\n",
        "  # Plot.\n",
        "  figsize = (8, 5)\n",
        "  plt.rcParams['figure.dpi'] = 300\n",
        "  plt.rcParams['savefig.dpi'] = 300\n",
        "  plt.rc('font', **{'size': 6})\n",
        "  plt.figure(figsize=figsize)\n",
        "\n",
        "  colors = [v for k, v in char_colors]\n",
        "  X_2d = pca.transform(X)\n",
        "  fig, ax = plt.subplots()\n",
        "  pc_index = [0, 1]\n",
        "  x = X_2d[:, pc_index[0]]\n",
        "  y = X_2d[:, pc_index[1]]\n",
        "  char_to_color_index = lambda c: (\n",
        "      ord(c) - ord('a') if c in string.ascii_lowercase else 26 + (ord(c) - ord('A'))) % len(colors)\n",
        "  ax.scatter(x=x, y=y, c=[colors[char_to_color_index(labels[i][0])] for i in range(len(labels))])\n",
        "\n",
        "  anno_char = collections.defaultdict(list)\n",
        "  for i, txt in enumerate(labels):\n",
        "    anno_char[txt[0]].append([x[i], y[i]])\n",
        "  for char, coords in anno_char.items():\n",
        "    ax.annotate(char, np.array(coords).mean(axis=0) + np.array([0.01, 0]), fontsize=12, weight='bold', color='white')\n",
        "    ax.annotate(char, np.array(coords).mean(axis=0) - np.array([0.03, 0]), fontsize=12, weight='bold', color='white')\n",
        "    ax.annotate(char, np.array(coords).mean(axis=0) + np.array([0, 0.03]), fontsize=12, weight='bold', color='white')\n",
        "    ax.annotate(char, np.array(coords).mean(axis=0) - np.array([0, 0.03]), fontsize=12, weight='bold', color='white')\n",
        "    ax.annotate(char, np.array(coords).mean(axis=0), fontsize=12)\n",
        " # task = 'Spelling Correction with Context'\n",
        "  #task = 'Word Search'\n",
        "  #task = 'Spelling Correction'\n",
        "  #task = 'Unscramble'\n",
        "  #task = 'Unit Conversion'\n",
        "  task = 'Reversal'\n",
        "  plt.title(f'Character Representations from {task} IIT Model (Encoder Layer 1)')\n",
        "  plt.xlabel(f'Principal Component {pc_index[0] + 1}')\n",
        "  plt.ylabel(f'Principal Component {pc_index[1] + 1}')"
      ],
      "metadata": {
        "id": "W83QZelrTLtl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Utils for processing IIT data for evaluation\n",
        "\n",
        "def parse_tsv_line_with_inv_anno(line, feature_to_column):\n",
        "  if not isinstance(line, str):\n",
        "    line = line['text']\n",
        "  parsed = line.strip().split('\\t')\n",
        "  return {k: parsed[v] if 'inv' not in k or\n",
        "                          'inv_val' in k or\n",
        "                          'inv_label' in k\n",
        "              else int(parsed[v])\n",
        "          for k, v in feature_to_column.items()}\n",
        "\n",
        "\n",
        "def parse_tsv_line_with_length_preserving_interchange_inv_anno(line, args):\n",
        "  feature_to_column, length_indexed_base_examples = args\n",
        "  parsed = line['text'].strip().split('\\t')\n",
        "  # replace the input inv locations with ones from a in-vocab word of the\n",
        "  # same length.\n",
        "  if parsed[2] == 'unit_conversion':\n",
        "    key = (parsed[0].replace(parsed[3], '{feature}'), len(parsed[3]))\n",
        "  else:\n",
        "    key = len(parsed[3])\n",
        "  candidates = length_indexed_base_examples[key]\n",
        "  inv_id_examples = random.choice(candidates) if candidates else None\n",
        "  example = {k: parsed[v] if 'inv' not in k or 'inv_val' in k or 'inv_label' in k\n",
        "                          else int(parsed[v])\n",
        "                          for k, v in feature_to_column.items()}\n",
        "  # If can't find an equivalent input, return the example itself.\n",
        "  if inv_id_examples is None:\n",
        "    print('No substitution')\n",
        "    inv_id_examples = example\n",
        "  # Keep the inv_val matches the actual value, but update the locations and\n",
        "  # input to match in-domain intervention example.\n",
        "  example['input'] = example['input'].replace(\n",
        "      example['feature'], inv_id_examples['feature'])\n",
        "  for k in example:\n",
        "    if 'inv_loc' in k:\n",
        "      example[k] = inv_id_examples[k]\n",
        "  return example\n",
        "\n",
        "\n",
        "def index_examples_by_length(\n",
        "    anno_input_tsv_path, anno_feature_to_column, first_n_line=10240):\n",
        "  print(anno_input_tsv_path)\n",
        "  examples = parse_annotated_examples(\n",
        "      anno_input_tsv_path, first_n_line=first_n_line,\n",
        "      parse_fn=parse_tsv_line_with_inv_anno,\n",
        "      parse_fn_args=anno_feature_to_column)\n",
        "  length_indexed_examples = collections.defaultdict(list)\n",
        "  # Index by input length.\n",
        "  for exp in examples:\n",
        "    length_indexed_examples[len(exp['feature'])].append(exp)\n",
        "  print(f'Indexed {len(examples)} examples into {len(length_indexed_examples)} keys.')\n",
        "  return length_indexed_examples"
      ],
      "metadata": {
        "id": "LrJ631eYYZO2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Eval interchange intervention accuracy\n",
        "\n",
        "alphanum = set(string.ascii_letters + string.digits)\n",
        "\n",
        "def eval_with_interchange_interventions(\n",
        "    model, eval_dataloader, data_config,\n",
        "    iit_layer=1, iit_dim=16, num_beams=None, num_return_sequences=1):\n",
        "  model.eval()\n",
        "  causal_abstraction = TransformerCausalAbstraction(model).to(device)\n",
        "  causal_abstraction.eval()\n",
        "\n",
        "  eval_outputs = {}\n",
        "  metrics = collections.defaultdict(int)\n",
        "  inputs_to_outputs = {}\n",
        "  for step, batch in enumerate(eval_dataloader):\n",
        "    for key in batch:\n",
        "      if key.startswith('inv_') and not isinstance(batch[key], list):\n",
        "        batch[key] = batch[key].tolist()\n",
        "    base_locations = [\n",
        "        (b_i, iit_layer,\n",
        "         batch[f'inv_loc_{inv_i}'][b_i],\n",
        "         batch[f'inv_loc_char_begin_{inv_i}'][b_i] * 16, # iit_dim\n",
        "         batch[f'inv_loc_char_end_{inv_i}'][b_i] * 16)   # iit_dim\n",
        "        for b_i in range(len(batch['input']))\n",
        "        for inv_i in range(data_config['max_num_anno'])\n",
        "        if batch[f'inv_loc_{inv_i}'][b_i] >= 0 and\n",
        "           batch[f'inv_val_{inv_i}'][b_i] in alphanum]\n",
        "    inv_values = [torch.mean(torch.from_numpy(\n",
        "              np.stack([v for vs in char_to_vec[c].values() for v in vs], axis=0)), dim=0).to(device)\n",
        "              for b_i in range(len(batch['input']))\n",
        "              for inv_i, c in enumerate([\n",
        "                  batch[f'inv_val_{i}'][b_i] for i in range(data_config['max_num_anno'])\n",
        "              if batch[f'inv_loc_{i}'][b_i] >= 0 and\n",
        "                  batch[f'inv_val_{i}'][b_i] in alphanum])]\n",
        "\n",
        "    assert len(base_locations) == len(inv_values)\n",
        "    base_input_batch = tokenizer(batch['input'], return_tensors='pt', padding=\"max_length\",\n",
        "                max_length=data_config['max_input_seq_len'], truncation=True)\n",
        "    for key in base_input_batch:\n",
        "      base_input_batch[key] = base_input_batch[key].to(device)\n",
        "\n",
        "    causal_abstraction.encoder.set_interchange_interventions(\n",
        "          None, None, None, None,\n",
        "          inv_values=inv_values, inv_value_locations=base_locations)\n",
        "    interventions = causal_abstraction.encoder.get_interchange_interventions()\n",
        "\n",
        "    inv_outputs = causal_abstraction.generate(\n",
        "        input_ids=base_input_batch['input_ids'],\n",
        "        attention_mask=base_input_batch['attention_mask'],\n",
        "        max_length=data_config['max_output_seq_len'],\n",
        "        num_beams=num_beams, num_return_sequences=num_return_sequences)\n",
        "    causal_abstraction.encoder.reset_interchange_interventions()\n",
        "    pred_text_batch = tokenizer.batch_decode(inv_outputs, skip_special_tokens=True)\n",
        "\n",
        "    base_outputs = model.generate(\n",
        "        input_ids=base_input_batch['input_ids'],\n",
        "        attention_mask=base_input_batch['attention_mask'],\n",
        "        max_length=data_config['max_output_seq_len'],\n",
        "        num_beams=num_beams, num_return_sequences=num_return_sequences)\n",
        "    base_pred_text_batch = tokenizer.batch_decode(base_outputs, skip_special_tokens=True)\n",
        "    for b_i in range(len(batch['input'])):\n",
        "      source_input_text = batch['feature_val'][b_i]\n",
        "      label_text = batch['label'][b_i]\n",
        "      iit_top1_pred = pred_text_batch[b_i * num_return_sequences]\n",
        "      iit_topk_preds = pred_text_batch[b_i * num_return_sequences: (b_i + 1) * num_return_sequences]\n",
        "      base_top1_pred = base_pred_text_batch[b_i * num_return_sequences]\n",
        "      base_topk_preds = base_pred_text_batch[b_i * num_return_sequences: (b_i + 1) * num_return_sequences]\n",
        "      iit_match_stats = compute_matching_stats(source_input_text, label_text, iit_top1_pred)\n",
        "      for k, v in iit_match_stats.items():\n",
        "        metrics[f'iit_{k}'] += v\n",
        "      if label_text in iit_topk_preds:\n",
        "        metrics['iit_match_top%d' % num_return_sequences] += 1\n",
        "      base_match_stats = compute_matching_stats(source_input_text, label_text, base_top1_pred)\n",
        "      for k, v in base_match_stats.items():\n",
        "        metrics[f'base_{k}'] += v\n",
        "      if label_text in base_topk_preds:\n",
        "        metrics['base_match_top%d' % num_return_sequences] += 1\n",
        "      eval_outputs[(batch['input'][b_i], batch['label'][b_i])] = {\n",
        "          'base': base_topk_preds,\n",
        "          'iit': iit_topk_preds,\n",
        "          'source_val': batch['feature_val'][b_i]}\n",
        "      metrics['total'] += 1\n",
        "  print(dict(metrics))\n",
        "  return metrics, eval_outputs\n",
        "\n",
        "\n",
        "def eval_with_clustered_representations(\n",
        "    model, test_iit_anno_tsv_path, data_config, char_to_vec,\n",
        "    base_iit_anno_tsv_path=None,\n",
        "    iit_layer=1, iit_dim=16, batch_size=128, num_beams=None,\n",
        "    num_return_sequences=1, num_repeat=1):\n",
        "  \"\"\"Evaluate IIT models with clustered character representations.\n",
        "\n",
        "    There are two ways to use the clustered representations, controlled by the\n",
        "    parameter `base_iit_anno_tsv_path`:\n",
        "      1) Directly replace aligned representations of each char with the\n",
        "        average pooled clustered representation, when `base_iit_anno_tsv_path`\n",
        "        is set to None.\n",
        "      2) Apply interchange interventions on a randomly sampled training examples,\n",
        "         where the intervention values are from the test examples, when\n",
        "         `base_iit_anno_tsv_path` is set to the path of the iit annotations.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the test dataset and apply interchange interventions on the base\n",
        "  # examples if `base_iit_anno_tsv_path` is provided.\n",
        "  anno_feature_to_column = {'input': 0, 'label': 1, 'task': 2, 'feature_val': 3}\n",
        "  anno_offset = len(anno_feature_to_column)\n",
        "  anno_feats = ['inv_loc', 'inv_loc_char_begin', 'inv_loc_char_end',\n",
        "                'inv_val', 'inv_out_begin', 'inv_out_end']\n",
        "  for inv_i in range(data_config['max_num_anno']):\n",
        "    anno_feature_to_column.update(\n",
        "        {f'{feat}_{inv_i}': anno_offset + feat_i + len(anno_feats) * inv_i\n",
        "          for feat_i, feat in enumerate(anno_feats)})\n",
        "  print(anno_feature_to_column)\n",
        "\n",
        "  parse_fn = parse_tsv_line_with_inv_anno\n",
        "  parse_fn_args = anno_feature_to_column\n",
        "  if base_iit_anno_tsv_path:\n",
        "    # Evaluate with interchange interventions on the base examples.\n",
        "    length_indexed_base_examples = index_examples_by_length(\n",
        "        base_iit_anno_tsv_path, anno_feature_to_column)\n",
        "    parse_fn = parse_tsv_line_with_length_preserving_interchange_inv_anno\n",
        "    parse_fn_args = (anno_feature_to_column, length_indexed_base_examples)\n",
        "\n",
        "  datasets.config.IN_MEMORY_MAX_SIZE = 1024**3  # 1G\n",
        "\n",
        "  acc_metrics = collections.defaultdict(int)\n",
        "  for _ in range(num_repeat):\n",
        "    text_anno_datasets = gen_seq2seq_text_dataset_from_tsv(\n",
        "          {'test': test_iit_anno_tsv_path},\n",
        "          parse_fn=parse_fn, parse_fn_args=parse_fn_args, keep_in_memory=True)\n",
        "    eval_dataloader = DataLoader(text_anno_datasets['test'], batch_size=batch_size)\n",
        "    metrics, eval_outputs = eval_with_interchange_interventions(\n",
        "        model, eval_dataloader, data_config,\n",
        "        iit_layer=iit_layer, iit_dim=iit_dim,\n",
        "        num_beams=num_beams, num_return_sequences=num_return_sequences)\n",
        "    for k in metrics:\n",
        "      acc_metrics[k] += metrics[k]\n",
        "  return acc_metrics, eval_outputs"
      ],
      "metadata": {
        "id": "Au30L_tQG4bp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown load models\n",
        "\n",
        "import string\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def load_model(model_name, ckpt, seq_len_config):\n",
        "  def is_char(model_name, side):\n",
        "    return ('char' in model_name and side in model_name) or 'byt5' in model_name\n",
        "  model = T5ForConditionalGeneration.from_pretrained(\n",
        "      os.path.join(MODEL_DIR, model_name, ckpt))\n",
        "  model = model.to(device)\n",
        "  if 'byt5' not in model_name:\n",
        "    char_tokenizer = copy.deepcopy(t5_default_tokenizer)\n",
        "    if 'unit_conversion' in model_name:\n",
        "      char_tokenizer.add_special_tokens(\n",
        "          {'additional_special_tokens': [c for c in string.digits + '.']})\n",
        "    else:\n",
        "      if 'spelling_correction_contextual' in model_name or 'word_search' in model_name:\n",
        "        char_tokenizer = T5Tokenizer(\n",
        "            os.path.join(MODEL_DIR, 't5_char_spiece.model'),\n",
        "            eos_token=tokenizer.eos_token,\n",
        "            unk_token=tokenizer.unk_token, pad_token=tokenizer.pad_token,\n",
        "            extra_ids=100,\n",
        "            additional_special_tokens=tokenizer.additional_special_tokens)\n",
        "      else:\n",
        "        char_tokenizer.add_special_tokens(\n",
        "            {'additional_special_tokens': [chr(i) for i in range(128) if chr(i) in VOCAB]})\n",
        "  else:\n",
        "    char_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
        "  source_tokenizer = char_tokenizer if is_char(model_name, 'src') else None\n",
        "  target_tokenizer = char_tokenizer if is_char(model_name, 'trg') else None\n",
        "  print('SOURCE_TOKENIZER:', source_tokenizer)\n",
        "  print('TARGET_TOKENIZER:', target_tokenizer)\n",
        "  max_input_seq_len = seq_len_config['max_src_char'] if is_char(model_name, 'src') else seq_len_config['max_src_token']\n",
        "  max_output_seq_len = seq_len_config['max_trg_char'] if is_char(model_name, 'trg') else seq_len_config['max_trg_token']\n",
        "  return model, (source_tokenizer, target_tokenizer), (max_input_seq_len, max_output_seq_len)"
      ],
      "metadata": {
        "id": "nSWvKDJh7fhQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks"
      ],
      "metadata": {
        "id": "kSMla9IBkI-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_TO_DATASETS = {\n",
        "    'reversal': {\n",
        "        'train': 'data/reversal_train.tsv',\n",
        "        'val': 'data/reversal_val.tsv',\n",
        "        'test_iv': 'data/reversal_test_iv.tsv',\n",
        "        'test_oov': 'data/reversal_test_oov.tsv',\n",
        "        'seq_length': {'max_src_token': 24, 'max_src_char': 24, 'max_trg_token': 16, 'max_trg_char': 16},\n",
        "    },\n",
        "    'unit_conversion': {\n",
        "        'train': 'data/unit_conversion_train.tsv',\n",
        "        'val': 'data/unit_conversion_val.tsv',\n",
        "        'test_iv': 'data/unit_conversion_test_iv.tsv',\n",
        "        'test_oov': 'data/unit_conversion_test_oov.tsv',\n",
        "        'seq_length': {'max_src_token': 16, 'max_src_char': 48, 'max_trg_token': 16, 'max_trg_char': 16},\n",
        "    },\n",
        "    'unscramble': {\n",
        "        'train': 'data/unscramble_train.tsv',\n",
        "        'val': 'data/unscramble_val.tsv',\n",
        "        'test_iv': 'data/unscramble_test_iv.tsv',\n",
        "        'test_oov': 'data/unscramble_test_oov.tsv',\n",
        "        'seq_length': {'max_src_token': 24, 'max_src_char': 24, 'max_trg_token': 16, 'max_trg_char': 16},\n",
        "    },\n",
        "    'spelling_correction': {\n",
        "        'train': 'data/spelling_correction_train.tsv',\n",
        "        'val': 'data/spelling_correction_val.tsv',\n",
        "        'test_iv': 'data/spelling_correction_test_iv.tsv',\n",
        "        'test_oov': 'data/spelling_correction_test_oov.tsv',\n",
        "        'test_real': 'data/spelling_correction_test_real.tsv',\n",
        "        'seq_length': {'max_src_token': 24, 'max_src_char': 24, 'max_trg_token': 16, 'max_trg_char': 16},\n",
        "    },\n",
        "    'spelling_correction_contextual': {\n",
        "        'train': 'data/spelling_correction_contextual_train.tsv',\n",
        "        'val': 'data/spelling_correction_contextual_val.tsv',\n",
        "        'test_context_independent': 'data/spelling_correction_contextual_test_context_independent.tsv',\n",
        "        'test_context_dependent': 'data/spelling_correction_contextual_test_context_dependent.tsv',\n",
        "        'seq_length': {'max_src_token': 48, 'max_src_char': 64, 'max_trg_token': 48, 'max_trg_char': 64},\n",
        "    },\n",
        "    'word_search': {\n",
        "        'train': 'data/word_search_train.tsv',\n",
        "        'val': 'data/word_search_val.tsv',\n",
        "        'test_oov': 'data/word_search_test_oov.tsv',\n",
        "        'test_paraphrase': 'data/word_search_test_paraphrase.tsv',\n",
        "        'test_overlap': 'data/word_search_test_overlap.tsv',\n",
        "        'test_paraphrase_overlap': 'data/word_search_test_paraphrase_overlap.tsv',\n",
        "        'seq_length': {'max_src_token': 48, 'max_src_char': 128, 'max_trg_token': 8, 'max_trg_char': 16},\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "tJjEnv3MCF0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating IIT Datasets"
      ],
      "metadata": {
        "id": "HkH471rgZLdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_TO_IIT_DATA = {\n",
        "    'reversal': {\n",
        "        'iit_config': {'max_num_inv': 16, 'max_num_anno': 16, 'include_space': False, 'eval_with_separate_base': True},\n",
        "    },\n",
        "    'unit_conversion': {\n",
        "        'iit_config': {'max_num_inv': 16, 'max_num_anno': 16, 'include_space': False, 'eval_with_separate_base': False},\n",
        "    },\n",
        "    'unscramble': {\n",
        "       'iit_config': {'max_num_inv': 16, 'max_num_anno': 16, 'include_space': False, 'eval_with_separate_base': True},\n",
        "    },\n",
        "    'spelling_correction': {\n",
        "       'iit_config': {'max_num_inv': 16, 'max_num_anno': 16, 'include_space': False, 'eval_with_separate_base': True},\n",
        "    },\n",
        "    'spelling_correction_contextual': {\n",
        "       'iit_config': {'max_num_inv': 64, 'max_num_anno': 64, 'include_space': True, 'eval_with_separate_base': False},\n",
        "    },\n",
        "    'word_search': {\n",
        "       'iit_config': {'max_num_inv': 24, 'max_num_anno': 24, 'include_space': False, 'eval_with_separate_base': False},\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "BQdJ0oyTByum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Annotate the location and value of each character\n",
        "task_name = 'reversal'\n",
        "base_input_path = os.path.join(DATA_DIR, '%s.tsv')\n",
        "\n",
        "anno_dataset_split_to_path = gen_character_annotation_data(\n",
        "    base_input_path, task_name,\n",
        "    inv_config=TASK_TO_IIT_DATA[task_name]['iit_config'],\n",
        "    splits=['train'])"
      ],
      "metadata": {
        "id": "fxrBHNQkZCeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate IIT Dataset\n",
        "\n",
        "anno_input_tsv_path = anno_dataset_split_to_path['train']\n",
        "output_tsv_path = anno_dataset_split_to_path['train'].replace('_char_anno.tsv', '_iit_examples.tsv')\n",
        "print(anno_input_tsv_path)\n",
        "print(output_tsv_path)\n",
        "\n",
        "inv_examples = gen_inv_examples(anno_input_tsv_path, output_tsv_path, 100_000, task_name)\n",
        "#test_gen_inv_example()"
      ],
      "metadata": {
        "id": "Vm8wYWC8ZqGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_TO_IIT_DATA[task_name]['iit_train'] = output_tsv_path"
      ],
      "metadata": {
        "id": "pEU7RWq_7SVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "cXhNewcWj6EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import string\n",
        "\n",
        "char_set = 'ascii' if task_name != 'unit_conversion' else 'digit'\n",
        "\n",
        "if char_set == 'digit':\n",
        "  special_tokens = string.digits + '.'\n",
        "  char_tokenizer = copy.deepcopy(t5_default_tokenizer)\n",
        "  char_tokenizer.add_special_tokens({'additional_special_tokens': [c for c in special_tokens]})\n",
        "elif char_set == 'ascii':\n",
        "  char_tokenizer = T5Tokenizer(\n",
        "    os.path.join('tokenizers/t5_char_spiece.model'),\n",
        "    eos_token=tokenizer.eos_token,\n",
        "    unk_token=tokenizer.unk_token, pad_token=tokenizer.pad_token,\n",
        "    extra_ids=100,\n",
        "    additional_special_tokens=tokenizer.additional_special_tokens)\n",
        "\n",
        "\n",
        "print(f'char_set=\"{char_set}\"')\n",
        "print([VOCAB[i] for i in char_tokenizer('test xxx').input_ids])\n",
        "print([VOCAB[i] for i in char_tokenizer('0.12 333').input_ids])"
      ],
      "metadata": {
        "id": "c6VZDnH5eFe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "USE_BYT5_TOKENIZER = False\n",
        "CHAR_TOKENIZE_TARGET = False\n",
        "CHAR_TOKENIZE_SOURCE = False\n",
        "if USE_BYT5_TOKENIZER:\n",
        "  CHAR_TOKENIZE_TARGET = True\n",
        "  CHAR_TOKENIZE_SOURCE = True\n",
        "\n",
        "print(f'CHAR_TOKENIZE_TARGET={CHAR_TOKENIZE_TARGET}')\n",
        "print(f'CHAR_TOKENIZE_SOURCE={CHAR_TOKENIZE_SOURCE}')\n",
        "print(f'USE_BYT5_TOKENIZER={USE_BYT5_TOKENIZER}')\n",
        "\n",
        "source_tokenizer = t5_default_tokenizer\n",
        "target_tokenizer = t5_default_tokenizer\n",
        "if USE_BYT5_TOKENIZER:\n",
        "  byt5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
        "  source_tokenizer = byt5_tokenizer\n",
        "  target_tokenizer = byt5_tokenizer\n",
        "  VOCAB = byt5_tokenizer.convert_ids_to_tokens(range(270))\n",
        "  print('Use ByT5 tokenizer.')\n",
        "if CHAR_TOKENIZE_SOURCE and not USE_BYT5_TOKENIZER:\n",
        "  source_tokenizer = char_tokenizer\n",
        "  print('Use char tokenizer for source.')\n",
        "if CHAR_TOKENIZE_TARGET and not USE_BYT5_TOKENIZER:\n",
        "  target_tokenizer = char_tokenizer\n",
        "  print('Use char tokenizer for target.')\n",
        "\n",
        "task_name = 'reversal'\n",
        "trainval_datasets, _ = load_datasets(\n",
        "    task_name, source_tokenizer=source_tokenizer, target_tokenizer=target_tokenizer)"
      ],
      "metadata": {
        "id": "TrSDwOwVHbBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IIT dataset\n",
        "ENABLE_IIT = True\n",
        "\n",
        "iit_dataset = gen_iit_dataset_from_tsv(task_name)"
      ],
      "metadata": {
        "id": "TW8hg8oE1wM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_batch_size = 16\n",
        "eval_batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(trainval_datasets['train'], shuffle=True,\n",
        "                                batch_size=training_batch_size)\n",
        "val_dataloader = DataLoader(trainval_datasets['val'], shuffle=True,\n",
        "                                      batch_size=eval_batch_size)\n",
        "iit_dataloader = DataLoader(iit_dataset['train'], shuffle=True,\n",
        "                            batch_size=training_batch_size)"
      ],
      "metadata": {
        "id": "-PfIwvORIJDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import collections\n",
        "\n",
        "try:\n",
        "  del model\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "print(task_name, 'training_batch_size=%d' % training_batch_size)\n",
        "print(f\"SEQ_LENGTH={TASK_TO_DATASETS[task_name]['seq_length']}\")\n",
        "print(f'CHAR_TOKENIZE_TARGET={CHAR_TOKENIZE_TARGET}')\n",
        "print(f'CHAR_TOKENIZE_SOURCE={CHAR_TOKENIZE_SOURCE}')\n",
        "\n",
        "\n",
        "restore_ckpt = False\n",
        "\n",
        "if not restore_ckpt:\n",
        "  start_epoch = 0\n",
        "  if not USE_BYT5_TOKENIZER:\n",
        "    # T5\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"t5-small\", cache_dir=MODEL_DIR)\n",
        "    #model = T5ForConditionalGeneration(config=model.config)\n",
        "  else:\n",
        "    # ByT5\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"google/byt5-small\", cache_dir=MODEL_DIR)\n",
        "    print('Use pre-trained byt5-small.')\n",
        "else:\n",
        "  start_epoch = 20\n",
        "  pretrain_task_name = task_name\n",
        "  ckpt = f'ckpt-ep{start_epoch}'\n",
        "  print(f'Resume from {pretrain_task_name} {ckpt}')\n",
        "  model = T5ForConditionalGeneration.from_pretrained(\n",
        "      os.path.join(MODEL_DIR, pretrain_task_name, ckpt))\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "num_epochs = 40\n",
        "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
        "# Need to keep num_training_steps the same for the same learning rate schedule.\n",
        "num_training_steps = (num_epochs + 20) * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=1_000,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "base_factor, inv_factor = 1.0, float(ENABLE_IIT)\n",
        "print(f'base_factor={base_factor} inv_factor={inv_factor}')\n",
        "\n",
        "iit_layer = 1\n",
        "char_dim = 16\n",
        "location_map_fn = lambda x: add_batch_layer_dim_location_map_fn(x, iit_layer, char_dim)\n",
        "if ENABLE_IIT:\n",
        "  print(f'iit_layer={iit_layer} char_dim={char_dim}')\n",
        "  enable_iit = True\n",
        "\n",
        "metrics_default_value = {'token_accuracy': 0, 'loss': 0, 'sequence_accuracy': 0}\n",
        "\n",
        "causal_abstraction = TransformerCausalAbstraction(model)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch = epoch + start_epoch\n",
        "  if ENABLE_IIT:\n",
        "    iit_dataloader_itrs = [iter(iit_dataloader)]\n",
        "  model.train()\n",
        "  for step, input_batch in enumerate(train_dataloader):\n",
        "    iit_batch = []\n",
        "    if ENABLE_IIT:\n",
        "      try:\n",
        "        iit_batch = ([next(iit_dataloader_itrs[i]) for i in range(len(iit_dataloader_itrs))])\n",
        "      except StopIteration:\n",
        "        iit_dataloader_itrs = [iter(iit_dataloader)]\n",
        "        iit_batch = ([next(iit_dataloader_itrs[i]) for i in range(len(iit_dataloader_itrs))])\n",
        "    for k in input_batch:\n",
        "      input_batch[k] = input_batch[k].to(device)\n",
        "    loss, outputs, iit_outputs = iit_train_step(\n",
        "        causal_abstraction, input_batch, iit_batch,\n",
        "        location_map_fn, base_factor, inv_factor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    if step % 500 == 0:\n",
        "      metrics = compute_metrics(([outputs.logits.detach().cpu().numpy()],\n",
        "                                  input_batch['labels'].cpu().numpy()))\n",
        "      # iit metrics only work if there is one iit dataset\n",
        "      iit_metrics = metrics_default_value\n",
        "      if iit_batch:\n",
        "        iit_metrics = compute_metrics(([iit_outputs.logits.detach().cpu().numpy()],\n",
        "                                       iit_batch[0]['labels'].cpu().numpy()))\n",
        "        iit_metrics['loss'] = iit_outputs.loss.detach().cpu().numpy()\n",
        "      print('Epoch %d Step %d: Loss %.4f Loss_iit %.4f Acc %.4f Acc_iit %.4f LR %.2E' % (\n",
        "          epoch, step, metrics['loss'],\n",
        "          iit_metrics['loss'],\n",
        "          metrics['token_accuracy'],\n",
        "          iit_metrics['token_accuracy'],\n",
        "          lr_scheduler.get_last_lr()[0]))\n",
        "    if step % 10000 == 0:\n",
        "      model.eval()\n",
        "      train_metrics = run_eval(model, train_dataloader, first_n_batch=16)\n",
        "      val_metrics = run_eval(model, val_dataloader, first_n_batch=16)\n",
        "      model.train()\n",
        "      print('Epoch %d Step %d: TRAIN Loss %.4f Accuracy %.4f VAL Accuracy %.4f' % (\n",
        "            epoch, step, train_metrics['loss'], train_metrics['token_accuracy'],\n",
        "            val_metrics['token_accuracy']))\n",
        "  model.save_pretrained(os.path.join(MODEL_DIR, task_name, 'ckpt-ep%d' % (epoch+1)))\n",
        "  print('Checkpoint saved at %s' % os.path.join(MODEL_DIR, task_name, 'ckpt-ep%d' % (epoch+1)))\n",
        "  # run eval\n",
        "  model.eval()\n",
        "  train_metrics = run_eval(model, train_dataloader, first_n_batch=16)\n",
        "  val_metrics = run_eval(model, val_dataloader, first_n_batch=100)\n",
        "  for key in train_metrics:\n",
        "    train_metrics[key] = float(np.array(train_metrics[key]).mean())\n",
        "  print('Epoch %d Done: TRAIN Loss %.4f Accuracy %.4f VAL Accuracy %.4f' % (\n",
        "          epoch, train_metrics['loss'], train_metrics['token_accuracy'],\n",
        "          val_metrics['token_accuracy']))"
      ],
      "metadata": {
        "id": "3g1Z69PcVcvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "_JecXc8DC2ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_TO_METRICS = {\n",
        "    'reversal': ['match_fullstring'],\n",
        "    'unit_conversion': ['match_fullstring'],\n",
        "    'unscramble': ['match_fullstring', 'match_anagram_valid'],\n",
        "    'spelling_correction': ['match_fullstring', 'match_spelling_correction'],\n",
        "    'spelling_correction_contextual':  ['match_fullstring'],\n",
        "    'word_search': ['match_fullstring'],\n",
        "}"
      ],
      "metadata": {
        "id": "-EY56SyOCdTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_task = 'reversal'\n",
        "model_name = 'reversal_subword_iit'"
      ],
      "metadata": {
        "id": "8oLG46HNCs_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_eval_outputs = {}\n",
        "for split, test_data_tsv_path in TASK_TO_DATASETS[eval_task].items():\n",
        "  if not split.startswith('test'):\n",
        "    continue\n",
        "  model, (source_tokenizer, target_tokenizer), max_seq_len = load_model(\n",
        "      model_name, '', TASK_TO_DATASETS[eval_task]['seq_length'])\n",
        "  test_datasets = gen_seqvc2seq_dataset_from_tsv(\n",
        "    {'test': test_data_tsv_path}, {'input': 0, 'label': 1}, max_seq_len,\n",
        "    source_tokenizer=source_tokenizer, target_tokenizer=target_tokenizer)\n",
        "  eval_outputs = eval_topk_accuracy(\n",
        "      model, test_datasets['test'], max_seq_len[1],\n",
        "      target_tokenizer=target_tokenizer if 'byt5' in model_name else None,\n",
        "      batch_size=128)\n",
        "  model_to_eval_outputs[(split, model_name)] = eval_outputs"
      ],
      "metadata": {
        "id": "lve11z2IZMPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
